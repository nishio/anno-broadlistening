{
  "name": "Recursive Public, Agenda Setting",
  "question": "\u4eba\u985e\u304c\u4eba\u5de5\u77e5\u80fd\u3092\u958b\u767a\u30fb\u5c55\u958b\u3059\u308b\u4e0a\u3067\u3001\u6700\u512a\u5148\u3059\u3079\u304d\u8ab2\u984c\u306f\u4f55\u3067\u3057\u3087\u3046\u304b\uff1f",
  "input": "example-polis",
  "model": "gpt-3.5-turbo",
  "extraction": {
    "workers": 3,
    "limit": 12,
    "properties": [],
    "categories": {},
    "category_batch_size": 5,
    "source_code": "import concurrent.futures\nimport json\nimport logging\nimport re\n\nimport pandas as pd\nfrom tqdm import tqdm\n\nfrom services.category_classification import classify_args\nfrom services.llm import request_to_chat_openai\nfrom services.parse_json_list import parse_response\n\nfrom utils import update_progress\n\nCOMMA_AND_SPACE_AND_RIGHT_BRACKET = re.compile(r\",\\s*(\\])\")\n\n\ndef _validate_property_columns(\n    property_columns: list[str], comments: pd.DataFrame\n) -> None:\n    if not all(property in comments.columns for property in property_columns):\n        raise ValueError(\n            f\"Properties {property_columns} not found in comments. Columns are {comments.columns}\"\n        )\n\n\ndef extraction(config):\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/args.csv\"\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n\n    model = config[\"extraction\"][\"model\"]\n    prompt = config[\"extraction\"][\"prompt\"]\n    workers = config[\"extraction\"][\"workers\"]\n    limit = config[\"extraction\"][\"limit\"]\n    property_columns = config[\"extraction\"][\"properties\"]\n    _validate_property_columns(property_columns, comments)\n    try:\n        # test all comment-id can be parsed as int\n        list(map(int, comments[\"comment-id\"].values))\n    except Exception as e:\n        print(\n            f\"inputs/{config['input']}.csv \u306e comment-id \u306b\u6574\u6570\u3067\u306a\u3044\u3082\u306e\u304c\u542b\u307e\u308c\u3066\u3044\u307e\u3059\",\n            e,\n        )\n        raise e\n    comment_ids = (comments[\"comment-id\"].values)[:limit]\n    comments.set_index(\"comment-id\", inplace=True)\n    results = pd.DataFrame()\n    update_progress(config, total=len(comment_ids))\n\n    existing_arguments = set()\n\n    for i in tqdm(range(0, len(comment_ids), workers)):\n        batch = comment_ids[i : i + workers]\n        batch_inputs = [comments.loc[id][\"comment-body\"] for id in batch]\n        batch_results = extract_batch(batch_inputs, prompt, model, workers)\n        for comment_id, extracted_args in zip(batch, batch_results):\n            for j, arg in enumerate(extracted_args):\n                if arg not in existing_arguments:\n                    properties = {\n                        prop: comments.loc[comment_id][prop]\n                        for prop in property_columns\n                    }\n                    new_row = {\n                        \"arg-id\": f\"A{comment_id}_{j}\",\n                        \"comment-id\": int(comment_id),\n                        \"argument\": arg,\n                        **properties,\n                    }\n                    results = pd.concat(\n                        [results, pd.DataFrame([new_row])], ignore_index=True\n                    )\n                    existing_arguments.add(arg)\n        update_progress(config, incr=len(batch))\n    if results.shape == (0, 0):\n        raise RuntimeError(\"result is empty, maybe bad prompt\")\n\n    classification_categories = config[\"extraction\"][\"categories\"]\n    if classification_categories:\n        results = classify_args(results, config, workers)\n    results.to_csv(path, index=False)\n\n\nlogging.basicConfig(level=logging.ERROR)\n\n\ndef extract_batch(batch, prompt, model, workers):\n    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:\n        futures_with_index = [\n            (i, executor.submit(extract_arguments, input, prompt, model))\n            for i, input in enumerate(batch)\n        ]\n\n        done, not_done = concurrent.futures.wait(\n            [f for _, f in futures_with_index], timeout=30\n        )\n        results = [[] for _ in range(len(batch))]\n\n        for _, future in futures_with_index:\n            if future in not_done and not future.cancelled():\n                future.cancel()\n\n        for i, future in futures_with_index:\n            if future in done:\n                try:\n                    result = future.result()\n                    results[i] = result\n                except Exception as e:\n                    logging.error(f\"Task {future} failed with error: {e}\")\n                    results[i] = []\n        return results\n\n\ndef extract_by_llm(input, prompt, model):\n    messages = [\n        {\"role\": \"system\", \"content\": prompt},\n        {\"role\": \"user\", \"content\": input},\n    ]\n    response = request_to_chat_openai(messages=messages, model=model)\n    return response\n\n\ndef extract_arguments(input, prompt, model, retries=1):\n    messages = [\n        {\"role\": \"system\", \"content\": prompt},\n        {\"role\": \"user\", \"content\": input},\n    ]\n    try:\n        response = request_to_chat_openai(messages=messages, model=model, is_json=False)\n        items = parse_response(response)\n        items = filter(None, items)  # omit empty strings\n        return items\n    except json.decoder.JSONDecodeError as e:\n        print(\"JSON error:\", e)\n        print(\"Input was:\", input)\n        print(\"Response was:\", response)\n        print(\"Silently giving up on trying to generate valid list.\")\n        return []\n",
    "prompt": "/system\n\u3042\u306a\u305f\u306f\u5c02\u9580\u7684\u306a\u30ea\u30b5\u30fc\u30c1\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u3067\u3001\u6574\u7406\u3055\u308c\u305f\u8b70\u8ad6\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u4f5c\u6210\u3059\u308b\u304a\u624b\u4f1d\u3044\u3092\u3059\u308b\u5f79\u5272\u3067\u3059\u3002\n\u4eba\u5de5\u77e5\u80fd\u306b\u95a2\u3059\u308b\u516c\u958b\u5354\u8b70\u3092\u5b9f\u65bd\u3057\u305f\u72b6\u6cc1\u3092\u60f3\u5b9a\u3057\u3066\u3044\u307e\u3059\u3002\u4e00\u822c\u5e02\u6c11\u304b\u3089\u5bc4\u305b\u3089\u308c\u305f\u8b70\u8ad6\u306e\u4f8b\u3092\u63d0\u793a\u3057\u307e\u3059\u306e\u3067\u3001\u305d\u308c\u3089\u3092\u3088\u308a\u7c21\u6f54\u3067\u8aad\u307f\u3084\u3059\u3044\u5f62\u306b\u6574\u7406\u3059\u308b\u304a\u624b\u4f1d\u3044\u3092\u304a\u9858\u3044\u3057\u307e\u3059\u3002\u5fc5\u8981\u306a\u5834\u5408\u306f2\u3064\u306e\u5225\u500b\u306e\u8b70\u8ad6\u306b\u5206\u5272\u3059\u308b\u3053\u3068\u3082\u3067\u304d\u307e\u3059\u304c\u3001\u591a\u304f\u306e\u5834\u5408\u306f1\u3064\u306e\u8b70\u8ad6\u306b\u307e\u3068\u3081\u308b\u65b9\u304c\u671b\u307e\u3057\u3044\u3067\u3057\u3087\u3046\u3002\n\u7d50\u679c\u306f\u6574\u5f62\u3055\u308c\u305fJSON\u5f62\u5f0f\u306e\u6587\u5b57\u5217\u30ea\u30b9\u30c8\u3068\u3057\u3066\u8fd4\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\u8981\u7d04\u306f\u5fc5\u305a\u65e5\u672c\u8a9e\u3067\u4f5c\u6210\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\n/human\n\nAI\u30c6\u30af\u30ce\u30ed\u30b8\u30fc\u306f\u3001\u305d\u306e\u30e9\u30a4\u30d5\u30b5\u30a4\u30af\u30eb\u5168\u4f53\u306b\u304a\u3051\u308b\u74b0\u5883\u8ca0\u8377\u3092\u524a\u6e1b\u3059\u308b\u3053\u3068\u306b\u7126\u70b9\u3092\u5f53\u3066\u3066\u958b\u767a\u3055\u308c\u308b\u3079\u304d\u3067\u3059\u3002\n\n/ai\n\n[\n\"AI\u30c6\u30af\u30ce\u30ed\u30b8\u30fc\u306e\u74b0\u5883\u8ca0\u8377\u524a\u6e1b\u306b\u7126\u70b9\u3092\u5f53\u3066\u308b\u3079\u304d\"\n]\n\n/human\n\nAI\u306e\u80fd\u529b\u3001\u9650\u754c\u3001\u502b\u7406\u7684\u8003\u616e\u4e8b\u9805\u306b\u3064\u3044\u3066\u3001\u4e00\u822c\u5e02\u6c11\u3092\u6559\u80b2\u3059\u308b\u5354\u8abf\u7684\u306a\u53d6\u308a\u7d44\u307f\u304c\u5fc5\u8981\u3067\u3059\u3002\n\n/ai\n\n[\n\"AI\u306e\u80fd\u529b\u306b\u3064\u3044\u3066\u4e00\u822c\u5e02\u6c11\u3092\u6559\u80b2\u3059\u3079\u304d\",\n\"AI\u306e\u9650\u754c\u3068\u502b\u7406\u7684\u8003\u616e\u4e8b\u9805\u306b\u3064\u3044\u3066\u4e00\u822c\u5e02\u6c11\u3092\u6559\u80b2\u3059\u3079\u304d\"\n]\n\n/human\n\nAI\u306f\u30b9\u30de\u30fc\u30c8\u30db\u30fc\u30e0\u3084\u30d3\u30eb\u306e\u30a8\u30cd\u30eb\u30ae\u30fc\u52b9\u7387\u3068\u5c45\u4f4f\u8005\u306e\u5feb\u9069\u6027\u3092\u6700\u9069\u5316\u3067\u304d\u307e\u3059\u3002\n\n/ai\n\n[\n\"AI\u306f\u30b9\u30de\u30fc\u30c8\u30db\u30fc\u30e0\u3084\u30d3\u30eb\u306e\u30a8\u30cd\u30eb\u30ae\u30fc\u52b9\u7387\u3068\u5c45\u4f4f\u8005\u306e\u5feb\u9069\u6027\u3092\u6700\u9069\u5316\u3067\u304d\u308b\"\n]\n\n/human\n\nAI\u306f\u30a8\u30cd\u30eb\u30ae\u30fc\u30b0\u30ea\u30c3\u30c9\u3092\u6700\u9069\u5316\u3057\u3001\u7121\u99c4\u3084\u70ad\u7d20\u6392\u51fa\u3092\u524a\u6e1b\u3067\u304d\u307e\u3059\u3002\n\n/ai\n\n[\n\"AI\u306f\u30a8\u30cd\u30eb\u30ae\u30fc\u30b0\u30ea\u30c3\u30c9\u3092\u6700\u9069\u5316\u3057\u3066\u7121\u99c4\u3068\u70ad\u7d20\u6392\u51fa\u3092\u524a\u6e1b\u3067\u304d\u308b\"\n]",
    "model": "gpt-3.5-turbo"
  },
  "clustering": {
    "clusters": 3,
    "source_code": "\"\"\"Cluster the arguments using UMAP + HDBSCAN and GPT-4.\"\"\"\n\nfrom importlib import import_module\n\nimport numpy as np\nimport pandas as pd\nfrom janome.tokenizer import Tokenizer\n\nSTOP_WORDS = [\n    \"\u306e\",\n    \"\u306b\",\n    \"\u306f\",\n    \"\u3092\",\n    \"\u305f\",\n    \"\u304c\",\n    \"\u3067\",\n    \"\u3066\",\n    \"\u3068\",\n    \"\u3057\",\n    \"\u308c\",\n    \"\u3055\",\n    \"\u3042\u308b\",\n    \"\u3044\u308b\",\n    \"\u3082\",\n    \"\u3059\u308b\",\n    \"\u304b\u3089\",\n    \"\u306a\",\n    \"\u3053\u3068\",\n    \"\u3068\u3057\u3066\",\n    \"\u3044\u304f\",\n    \"\u306a\u3044\",\n]\nTOKENIZER = Tokenizer()\n\n\ndef clustering(config):\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/clusters.csv\"\n    arguments_df = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    arguments_array = arguments_df[\"argument\"].values\n\n    embeddings_df = pd.read_pickle(f\"outputs/{dataset}/embeddings.pkl\")\n    embeddings_array = np.asarray(embeddings_df[\"embedding\"].values.tolist())\n    clusters = config[\"clustering\"][\"clusters\"]\n\n    result = cluster_embeddings(\n        docs=arguments_array,\n        embeddings=embeddings_array,\n        metadatas={\n            \"arg-id\": arguments_df[\"arg-id\"].values,\n            \"comment-id\": arguments_df[\"comment-id\"].values,\n        },\n        min_cluster_size=clusters,\n        n_topics=clusters,\n    )\n    result.to_csv(path, index=False)\n\n\ndef tokenize_japanese(text):\n    return [\n        token.surface\n        for token in TOKENIZER.tokenize(text)\n        if token.surface not in STOP_WORDS\n    ]\n\n\ndef cluster_embeddings(\n    docs,\n    embeddings,\n    metadatas,\n    min_cluster_size=2,\n    n_components=2,\n    n_topics=6,\n):\n    # (!) we import the following modules dynamically for a reason\n    # (they are slow to load and not required for all pipelines)\n    SpectralClustering = import_module(\"sklearn.cluster\").SpectralClustering\n    HDBSCAN = import_module(\"hdbscan\").HDBSCAN\n    UMAP = import_module(\"umap\").UMAP\n    CountVectorizer = import_module(\"sklearn.feature_extraction.text\").CountVectorizer\n    BERTopic = import_module(\"bertopic\").BERTopic\n\n    umap_model = UMAP(\n        random_state=42,\n        n_components=n_components,\n    )\n    hdbscan_model = HDBSCAN(min_cluster_size=min_cluster_size)\n\n    vectorizer_model = CountVectorizer(tokenizer=tokenize_japanese)\n    topic_model = BERTopic(\n        umap_model=umap_model,\n        hdbscan_model=hdbscan_model,\n        vectorizer_model=vectorizer_model,\n        verbose=True,\n    )\n\n    # Fit the topic model.\n    _, __ = topic_model.fit_transform(docs, embeddings=embeddings)\n\n    n_samples = len(embeddings)\n    n_neighbors = min(n_samples - 1, 10)\n    spectral_model = SpectralClustering(\n        n_clusters=n_topics,\n        affinity=\"nearest_neighbors\",\n        n_neighbors=n_neighbors,  # Use the modified n_neighbors\n        random_state=42,\n    )\n    umap_embeds = umap_model.fit_transform(embeddings)\n    cluster_labels = spectral_model.fit_predict(umap_embeds)\n\n    result = topic_model.get_document_info(\n        docs=docs,\n        metadata={\n            **metadatas,\n            \"x\": umap_embeds[:, 0],\n            \"y\": umap_embeds[:, 1],\n        },\n    )\n\n    result.columns = [c.lower() for c in result.columns]\n    result = result[[\"arg-id\", \"x\", \"y\", \"probability\"]]\n    result[\"cluster-id\"] = cluster_labels\n\n    return result\n"
  },
  "intro": "\u3053\u306eAI\u751f\u6210\u30ec\u30dd\u30fc\u30c8\u306f\u3001Recursive Public\u30c1\u30fc\u30e0\u304c\u5b9f\u65bd\u3057\u305fPolis\u5354\u8b70\u306e\u30c7\u30fc\u30bf\u306b\u57fa\u3065\u3044\u3066\u3044\u307e\u3059\u3002",
  "output_dir": "example-polis",
  "embedding": {
    "model": "text-embedding-3-small",
    "source_code": "import os\n\nimport pandas as pd\nfrom dotenv import load_dotenv\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain_openai import AzureOpenAIEmbeddings\nfrom tqdm import tqdm\n\nload_dotenv(\"../../.env\")\n\nEMBDDING_MODELS = [\n    \"text-embedding-3-large\",\n    \"text-embedding-3-small\",\n]\n\n\ndef _validate_model(model):\n    if model not in EMBDDING_MODELS:\n        raise RuntimeError(\n            f\"Invalid embedding model: {model}, available models: {EMBDDING_MODELS}\"\n        )\n\n\ndef embed_by_openai(args, model):\n    if os.getenv(\"USE_AZURE\"):\n        embeds = AzureOpenAIEmbeddings(\n            model=model,\n            azure_endpoint=os.getenv(\"AZURE_EMBEDDING_ENDPOINT\"),\n        ).embed_documents(args)\n    else:\n        _validate_model(model)\n        embeds = OpenAIEmbeddings(model=model).embed_documents(args)\n    return embeds\n\n\ndef embedding(config):\n    model = config[\"embedding\"][\"model\"]\n\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/embeddings.pkl\"\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    embeddings = []\n    batch_size = 1000\n    for i in tqdm(range(0, len(arguments), batch_size)):\n        args = arguments[\"argument\"].tolist()[i: i + batch_size]\n        embeds = embed_by_openai(args, model)\n        embeddings.extend(embeds)\n    df = pd.DataFrame(\n        [\n            {\"arg-id\": arguments.iloc[i][\"arg-id\"], \"embedding\": e}\n            for i, e in enumerate(embeddings)\n        ]\n    )\n    df.to_pickle(path)\n"
  },
  "labelling": {
    "sample_size": 30,
    "source_code": "\"\"\"Create labels for the clusters.\"\"\"\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nfrom services.llm import request_to_chat_openai\nfrom utils import update_progress\n\n# TODO: \u30d7\u30ed\u30f3\u30d7\u30c8\u8a2d\u5b9a\u306e\u5916\u90e8\u5316\nBASE_SELECTION_PROMPT = \"\"\"\u30af\u30e9\u30b9\u30bf\u306b\u3064\u3051\u3089\u308c\u305f\u30e9\u30d9\u30eb\u540d\u3068\u3001\u7d10\u3065\u304f\u30c7\u30fc\u30bf\u70b9\u306e\u30c6\u30ad\u30b9\u30c8\u3092\u4e0e\u3048\u308b\u306e\u3067\u3001\n\u30e9\u30d9\u30eb\u540d\u3068\u95a2\u9023\u5ea6\u306e\u9ad8\u3044\u30c6\u30ad\u30b9\u30c8\u306eid\u30925\u3064\u51fa\u529b\u3057\u3066\u304f\u3060\u3055\u3044\n\n# \u6307\u793a\n* \u30e9\u30d9\u30eb\u3068\u5404\u30c7\u30fc\u30bf\u70b9\u306e\u30c6\u30ad\u30b9\u30c8\u3092\u78ba\u8a8d\u3057\u305f\u4e0a\u3067\u3001\u95a2\u9023\u5ea6\u306e\u9ad8\u3044id\u3092\u51fa\u529b\u3057\u3066\u304f\u3060\u3055\u3044\n* \u51fa\u529b\u306f\u30ab\u30f3\u30de\u533a\u5207\u308a\u3067\u3001\u30b9\u30da\u30fc\u30b9\u3092\u542b\u3081\u305a\u306b5\u3064\u306eid\u3092\u51fa\u529b\u3057\u3066\u4e0b\u3055\u3044\n* \u51fa\u529b\u7d50\u679c\u306f\u4eba\u9593\u304c\u95b2\u89a7\u3059\u308b\u306e\u3067\u3001\u4eba\u9593\u304c\u89e3\u91c8\u3057\u3084\u3059\u3044\u30c6\u30ad\u30b9\u30c8\u3092\u9078\u5b9a\u3057\u3066\u304f\u3060\u3055\u3044\n\n# \u51fa\u529b\u4f8b\nA199_0,A308_0,A134_2,A134_1,A123_0\n\n# \u30e9\u30d9\u30eb\u540d\n{label}\n\n# \u5404\u30c7\u30fc\u30bf\u70b9\u306e\u30c6\u30ad\u30b9\u30c8\n{args_text}\n\"\"\"\n\n\ndef select_relevant_ids_by_llm(prompt, model=\"gpt-4o\"):\n    messages = [{\"role\": \"user\", \"content\": prompt}]\n    try:\n        response = request_to_chat_openai(messages=messages, model=model)\n        selected_ids = response.strip().split(\",\")\n        return [id_str.strip() for id_str in selected_ids]\n    except Exception as e:\n        print(e)\n        return []\n\n\ndef select_representative_args(\n    cluster_args, label, cid, model=\"gpt-4o\", sampling_num=50\n):\n    arg_rows = cluster_args[cluster_args[\"cluster-id\"] == cid].sort_values(\n        by=\"probability\", ascending=False\n    )\n    # hdbscan\u306e\u30af\u30e9\u30b9\u30bf\u306b\u304a\u3051\u308b\u6240\u5c5e\u78ba\u7387(probability)\u304c\u9ad8\u3044\u9806\u306b\u53d6\u5f97\u3057\u3001\u4ee3\u8868\u30b3\u30e1\u30f3\u30c8\u306e\u5019\u88dc\u3068\u3059\u308b\n    top_rows = arg_rows.head(sampling_num)\n    args_text = \"\\n\".join(\n        [\n            f\"{row['arg-id']}: {row['argument']}\"\n            for _, (_, row) in enumerate(top_rows.iterrows())\n        ]\n    )\n    prompt = BASE_SELECTION_PROMPT.format(label=label, args_text=args_text)\n    selected_ids = select_relevant_ids_by_llm(prompt, model)\n    return selected_ids\n\n\ndef update_cluster_probability(config, arguments, clusters, labels):\n    cluster_args = arguments.merge(clusters, on=\"arg-id\", how=\"left\")\n    for _, row in labels.iterrows():\n        cid = row[\"cluster-id\"]\n        label = row[\"label\"]\n        selected_ids = select_representative_args(cluster_args, label, cid)\n        for id in selected_ids:\n            mask = cluster_args[\"arg-id\"] == id\n            clusters.loc[mask, \"probability\"] += 100\n    clusters.to_csv(f\"outputs/{config['output_dir']}/clusters.csv\", index=False)\n\n\ndef labelling(config):\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/labels.csv\"\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    clusters = pd.read_csv(f\"outputs/{dataset}/clusters.csv\")\n\n    results = pd.DataFrame()\n\n    sample_size = config[\"labelling\"][\"sample_size\"]\n    prompt = config[\"labelling\"][\"prompt\"]\n    model = config[\"labelling\"][\"model\"]\n\n    question = config[\"question\"]\n    cluster_ids = clusters[\"cluster-id\"].unique()\n\n    update_progress(config, total=len(cluster_ids))\n\n    for _, cluster_id in tqdm(enumerate(cluster_ids), total=len(cluster_ids)):\n        args_ids = clusters[clusters[\"cluster-id\"] == cluster_id][\"arg-id\"].values\n        args_ids = np.random.choice(\n            args_ids, size=min(len(args_ids), sample_size), replace=False\n        )\n        args_sample = arguments[arguments[\"arg-id\"].isin(args_ids)][\"argument\"].values\n\n        args_ids_outside = clusters[clusters[\"cluster-id\"] != cluster_id][\n            \"arg-id\"\n        ].values\n        args_ids_outside = np.random.choice(\n            args_ids_outside,\n            size=min(len(args_ids_outside), sample_size),\n            replace=False,\n        )\n        args_sample_outside = arguments[arguments[\"arg-id\"].isin(args_ids_outside)][\n            \"argument\"\n        ].values\n\n        label = generate_label(\n            question, args_sample, args_sample_outside, prompt, model\n        )\n        results = pd.concat(\n            [results, pd.DataFrame([{\"cluster-id\": cluster_id, \"label\": label}])],\n            ignore_index=True,\n        )\n        update_progress(config, incr=1)\n\n    results.to_csv(path, index=False)\n    update_cluster_probability(config, arguments, clusters, results)\n\n\ndef generate_label(question, args_sample, args_sample_outside, prompt, model):\n    outside = \"\\n * \" + \"\\n * \".join(args_sample_outside)\n    inside = \"\\n * \" + \"\\n * \".join(args_sample)\n    input = (\n        f\"\u8cea\u554f:\\n{question}\\n\\n\"\n        + f\"\u30af\u30e9\u30b9\u30bf\u5916\u90e8\u306e\u610f\u898b:\\n{outside}\\n\"\n        + f\"\u30af\u30e9\u30b9\u30bf\u5185\u90e8\u306e\u610f\u898b:\\n{inside}\"\n    )\n    messages = [{\"role\": \"user\", \"content\": prompt}, {\"role\": \"user\", \"content\": input}]\n    response = request_to_chat_openai(messages=messages, model=model)\n    return response\n",
    "prompt": "/system \n\n\u30af\u30e9\u30b9\u30bf\u5206\u6790\u306e\u7d50\u679c\u3092\u4e0e\u3048\u308b\u306e\u3067\u3001\u30af\u30e9\u30b9\u30bf\u306b\u3075\u3055\u308f\u3057\u3044\u30e9\u30d9\u30eb\u3092\u751f\u6210\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u8b70\u8ad6\u304b\u3089\u51fa\u305f\u610f\u898b\u30fb\u8981\u671b\u30fb\u6279\u5224\u3001\u30af\u30e9\u30b9\u30bf\u5185\u306e\u8b70\u8ad6\u306e\u30ea\u30b9\u30c8\u3001\u304a\u3088\u3073\u3053\u306e\u30af\u30e9\u30b9\u30bf\u5916\u306e\u8b70\u8ad6\u306e\u30ea\u30b9\u30c8\u304c\u4e0e\u3048\u308b\u306e\u3067\u30af\u30e9\u30b9\u30bf\u30fc\u3092\u8981\u7d04\u3059\u308b1\u3064\u306e\u30ab\u30c6\u30b4\u30ea\u30fc\u30e9\u30d9\u30eb\u3092\u4f5c\u6210\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\n\u8cea\u554f\u304b\u3089\u3059\u3067\u306b\u660e\u3089\u304b\u306a\u6587\u8108\u306f\u542b\u3081\u306a\u3044\uff08\u4f8b\u3048\u3070\u3001\u76f8\u8ac7\u306e\u8cea\u554f\u304c\u300c\u30d5\u30e9\u30f3\u30b9\u3067\u3069\u306e\u3088\u3046\u306a\u8ab2\u984c\u306b\u76f4\u9762\u3057\u3066\u3044\u308b\u304b\u300d\u306e\u3088\u3046\u306a\u3082\u306e\u3067\u3042\u308c\u3070\u3001\u30af\u30e9\u30b9\u30bf\u30fc\u306e\u30e9\u30d9\u30eb\u306b\u300c\u30d5\u30e9\u30f3\u30b9\u3067\u300d\u3068\u7e70\u308a\u8fd4\u3059\u5fc5\u8981\u306f\u306a\u3044\uff09\u3002\n\n\u30e9\u30d9\u30eb\u306f\u975e\u5e38\u306b\u7c21\u6f54\u3067\u306a\u3051\u308c\u3070\u306a\u3089\u305a\u3001\u30af\u30e9\u30b9\u30bf\u30fc\u3068\u305d\u306e\u5916\u5074\u306b\u3042\u308b\u5185\u5bb9\u3092\u533a\u5225\u3059\u308b\u306e\u306b\u5341\u5206\u306a\u6b63\u78ba\u3055\u3067\u306a\u3051\u308c\u3070\u306a\u3089\u306a\u3044\u3002\n\u30e9\u30d9\u30eb\u540d\u306f\u3001\u5fc5\u305a\u65e5\u672c\u8a9e\u3067\u8a18\u8ff0\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\n/human\n\n\u30b3\u30f3\u30b5\u30eb\u30c6\u30fc\u30b7\u30e7\u30f3\u306e\u8cea\u554f \u300c\u82f1\u56fd\u306eEU\u96e2\u8131\u6c7a\u5b9a\u306b\u969b\u3057\u3066EU\u306f\u3069\u306e\u3088\u3046\u306a\u5bfe\u51e6\u3092\u3059\u3079\u304d\u3060\u3068\u601d\u3044\u307e\u3059\u304b\uff1f\n\n\u95a2\u5fc3\u306e\u3042\u308b\u30af\u30e9\u30b9\u30bf\u30fc\u4ee5\u5916\u306e\u63d0\u6848\u3068\u8981\u671b\u306e\u4f8b\n\n * \u30a8\u30e9\u30b9\u30e0\u30b9\u30fb\u30d7\u30ed\u30b0\u30e9\u30e0\u304b\u3089\u306e\u9664\u5916\u306b\u3088\u308a\u3001\u6559\u80b2\u30fb\u6587\u5316\u4ea4\u6d41\u306e\u6a5f\u4f1a\u304c\u5236\u9650\u3055\u308c\u306a\u3044\u3088\u3046\u306b\u3059\u3079\u304d\u3002\n * \u56fd\u5883\u691c\u554f\u306e\u5f37\u5316\u306b\u3088\u308b\u65c5\u884c\u6642\u9593\u306e\u5ef6\u9577\u306b\u5bfe\u51e6\u3057\u3001\u901a\u52e4\u5ba2\u3084\u65c5\u884c\u5ba2\u306b\u5f71\u97ff\u3092\u8efd\u6e1b\u3059\u3079\u304d\u3002\n * \u74b0\u5883\u57fa\u6e96\u306b\u304a\u3051\u308b\u5354\u529b\u3092\u7dad\u6301\u3057\u3001\u6c17\u5019\u5909\u52d5\u3068\u95d8\u3046\u52aa\u529b\u3092\u5411\u4e0a\u3059\u3079\u304d\u3002\n * \u76f8\u4e92\u533b\u7642\u5354\u5b9a\u306e\u4e2d\u65ad\u305b\u305a\u306b\u3001\u60a3\u8005\u30b1\u30a2\u3092\u6e1b\u3089\u3055\u306a\u3044\u3088\u3046\u306b\u3059\u3079\u304d\u3002\n * Brexit\u95a2\u9023\u306e\u5909\u66f4\u306b\u3088\u308a\u3001\u5bb6\u65cf\u306e\u5c45\u4f4f\u6a29\u3084\u5e02\u6c11\u6a29\u306e\u7533\u8acb\u3092\u8907\u96d1\u306b\u3057\u306a\u3044\u3088\u3046\u306b\u3059\u3079\u304d\u3002\n * \u82f1\u56fd\u3068\u306e\u5171\u540c\u7814\u7a76\u6a5f\u4f1a\u3092\u7dad\u6301\u3057\u3001\u7814\u7a76\u306e\u8ab2\u984c\u306b\u53d6\u308a\u7d44\u3080\u4e16\u754c\u7684\u306a\u53d6\u308a\u7d44\u307f\u3092\u7dad\u6301\u3059\u3079\u304d\u3002\n * EU\u306e\u6587\u5316\u52a9\u6210\u30d7\u30ed\u30b0\u30e9\u30e0\u304b\u3089\u306e\u9664\u5916\u306e\u5f71\u97ff\u3092\u6e1b\u3089\u3057\u3001\u5275\u9020\u7684\u306a\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3092\u5236\u9650\u3057\u306a\u3044\u3088\u3046\u306b\u3059\u3079\u304d\u3002\n * EU\u306e\u8cc7\u91d1\u63d0\u4f9b\u306e\u55aa\u5931\u306e\u5f71\u97ff\u3092\u6e1b\u3089\u3057\u3001\u6148\u5584\u6d3b\u52d5\u3084\u30b3\u30df\u30e5\u30cb\u30c6\u30a3\u652f\u63f4\u304c\u5f8c\u9000\u3057\u306a\u3044\u3088\u3046\u306b\u3057\u3066\u307b\u3057\u3044\u3002\n * \u6d88\u8cbb\u8005\u4fdd\u8b77\u306e\u5f31\u4f53\u5316\u3055\u305b\u305a\u3001\u56fd\u5883\u3092\u8d8a\u3048\u305f\u7d1b\u4e89\u89e3\u6c7a\u306b\u30b3\u30df\u30c3\u30c8\u3057\u3066\u307b\u3057\u3044\u3002\n * \u82f1\u56fd\u306e\u30d7\u30ed\u306e\u97f3\u697d\u5bb6\u306eEU\u8af8\u56fd\u30c4\u30a2\u30fc\u3092\u5236\u9650\u305b\u305a\u3001\u30ad\u30e3\u30ea\u30a2\u306b\u5f71\u97ff\u3092\u4e0e\u3048\u306a\u3044\u3067\u307b\u3057\u3044\u3002\n\n\u30af\u30e9\u30b9\u30bf\u30fc\u5185\u90e8\u3067\u306e\u63d0\u6848\u306e\u4f8b\n\n * Brexit\u306b\u3088\u308b\u30b5\u30d7\u30e9\u30a4\u30c1\u30a7\u30fc\u30f3\u3078\u306e\u5f71\u97ff\u3092\u3068\u3069\u3081\u3001\u4f01\u696d\u306b\u3068\u3063\u3066\u30b3\u30b9\u30c8\u5897\u3068\u7d0d\u671f\u9045\u5ef6\u3092\u56de\u907f\u3059\u3079\u304d\u3002\n * \u30d6\u30ec\u30b0\u30b8\u30c3\u30c8\u306b\u3088\u308b\u5e02\u5834\u306e\u5909\u52d5\u3084\u6295\u8cc7\u30fb\u9000\u8077\u91d1\u306e\u4e0d\u78ba\u5b9f\u6027\u3092\u6e1b\u3089\u3057\u3066\u307b\u3057\u3044\u3002\n * \u65b0\u305f\u306a\u95a2\u7a0e\u3084\u901a\u95a2\u624b\u7d9a\u304d\u3092\u8003\u616e\u3057\u3001\u82f1\u56fd\u306f\u8f38\u51fa\u696d\u8005\u3068\u3057\u3066\u5229\u76ca\u7387\u306e\u4f4e\u4e0b\u306b\u5bfe\u51e6\u3059\u3079\u304d\u3002\n * \u30d6\u30ec\u30b0\u30b8\u30c3\u30c8\u5f8c\u3001\u4f01\u696d\u304cEU\u5e02\u5834\u5185\u306b\u3068\u3069\u307e\u308b\u305f\u3081\u306b\u4e8b\u696d\u3092\u79fb\u8ee2\u305b\u305a\u306b\u3001\u96c7\u7528\u3092\u5931\u308f\u306a\u3044\u3088\u3046\u306b\u3057\u3066\u307b\u3057\u3044\u3002\n * \u82f1\u56fd\u306f\u8f38\u5165\u54c1\u4fa1\u683c\u306e\u9ad8\u9a30\u306b\u3088\u308b\u751f\u6d3b\u8cbb\u306e\u5897\u52a0\u306b\u5bfe\u51e6\u3057\u3066\u307b\u3057\u3044\u3002\n * \u82f1\u56fd\u306e\u30cf\u30a4\u30c6\u30af\u7523\u696d\u3078\u306e\u6295\u8cc7\u3092\u7dad\u6301\u3057\u3001\u6280\u8853\u9769\u65b0\u3068\u96c7\u7528\u6a5f\u4f1a\u3092\u4fdd\u3064\u3079\u304d\u3002\n * \u65b0\u305f\u306a\u30d3\u30b6\u898f\u5236\u306b\u3088\u308b\u89b3\u5149\u5ba2\u306e\u6e1b\u5c11\u306b\u5099\u3048\u3001\u65b0\u305f\u306a\u63a5\u5ba2\u696d\u3078\u306e\u523a\u6fc0\u7b56\u3092\u8003\u3048\u308b\u3079\u304d\u3002\n * \u30dd\u30f3\u30c9\u4fa1\u5024\u306e\u4e0b\u843d\u306b\u3088\u308a\u8cfc\u8cb7\u529b\u304c\u4f4e\u4e0b\u306b\u5099\u3048\u3001\u65c5\u8cbb\u306e\u5897\u52a0\u306b\u5bfe\u51e6\u3057\u3066\u307b\u3057\u3044\u3002\n\n\n/ai \n\n\u8ca1\u52d9\u4e0a\u306e\u30de\u30a4\u30ca\u30b9\u5f71\u97ff\u3078\u306e\u5bfe\u51e6\u3092\u8003\u3048\u308b\u3079\u304d\n",
    "model": "gpt-3.5-turbo"
  },
  "takeaways": {
    "sample_size": 30,
    "source_code": "\"\"\"Create summaries for the clusters.\"\"\"\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nfrom services.llm import request_to_chat_openai\nfrom utils import update_progress\n\n\ndef takeaways(config):\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/takeaways.csv\"\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    clusters = pd.read_csv(f\"outputs/{dataset}/clusters.csv\")\n\n    results = pd.DataFrame()\n\n    sample_size = config[\"takeaways\"][\"sample_size\"]\n    prompt = config[\"takeaways\"][\"prompt\"]\n    model = config[\"takeaways\"][\"model\"]\n\n    model = config.get(\"model_takeaways\", config.get(\"model\", \"gpt3.5-turbo\"))\n    cluster_ids = clusters[\"cluster-id\"].unique()\n\n    update_progress(config, total=len(cluster_ids))\n\n    for _, cluster_id in tqdm(enumerate(cluster_ids), total=len(cluster_ids)):\n        args_ids = clusters[clusters[\"cluster-id\"] == cluster_id][\"arg-id\"].values\n        args_ids = np.random.choice(\n            args_ids, size=min(len(args_ids), sample_size), replace=False\n        )\n        args_sample = arguments[arguments[\"arg-id\"].isin(args_ids)][\"argument\"].values\n        label = generate_takeaways(args_sample, prompt, model)\n        results = pd.concat(\n            [results, pd.DataFrame([{\"cluster-id\": cluster_id, \"takeaways\": label}])],\n            ignore_index=True,\n        )\n        update_progress(config, incr=1)\n\n    results.to_csv(path, index=False)\n\n\ndef generate_takeaways(args_sample, prompt, model):\n    input = \"\\n\".join(args_sample)\n    messages = [{\"role\": \"user\", \"content\": prompt}, {\"role\": \"user\", \"content\": input}]\n    response = request_to_chat_openai(messages=messages, model=model)\n    return response\n",
    "prompt": "/system \n\n\u3042\u306a\u305f\u306f\u30d7\u30ed\u306e\u30ea\u30b5\u30fc\u30c1\u30fb\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u3067\u3001\u79c1\u306e\u4ed5\u4e8b\u3092\u624b\u4f1d\u3046\u3053\u3068\u304c\u3042\u306a\u305f\u306e\u4ed5\u4e8b\u3067\u3059\u3002\nX\u4e0a\u306e\u30dd\u30b9\u30c8\u306e\u30ea\u30b9\u30c8\u3092\u4e0e\u3048\u308b\u306e\u3067\u3001\u305d\u308c\u3089\u306e\u4e3b\u8981\u306a\u30dd\u30a4\u30f3\u30c8\u3092\u4e00\u3064\u304b\u4e8c\u3064\u306e\u6bb5\u843d\u3067\u307e\u3068\u3081\u3066\u304f\u3060\u3055\u3044\u3002\n\u5fdc\u7b54\u30c6\u30ad\u30b9\u30c8\u306f\u3001\u7c21\u6f54\u3067\u3001\u77ed\u304f\u3001\u8aad\u307f\u3084\u3059\u3044\u6587\u7ae0\u306b\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\u51fa\u529b\u306f\u5fc5\u305a\u65e5\u672c\u8a9e\u3067\u884c\u3063\u3066\u304f\u3060\u3055\u3044\u3002\n \n/human\n\n[\n  \"\u79c1\u306f\u9283\u66b4\u529b\u304c\u6211\u3005\u306e\u793e\u4f1a\u306b\u304a\u3051\u308b\u6df1\u523b\u306a\u516c\u8846\u885b\u751f\u306e\u5371\u6a5f\u3067\u3042\u308b\u3068\u5f37\u304f\u4fe1\u3058\u3066\u3044\u307e\u3059\u3002\",\n  \"\u3053\u306e\u554f\u984c\u3092\u5305\u62ec\u7684\u306a\u9283\u898f\u5236\u306b\u3088\u3063\u3066\u7dca\u6025\u306b\u5bfe\u51e6\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\",\n  \"\u79c1\u306f\u5168\u3066\u306e\u9283\u8cfc\u5165\u8005\u306b\u5bfe\u3059\u308b\u30e6\u30cb\u30d0\u30fc\u30b5\u30eb\u80cc\u666f\u8abf\u67fb\u306e\u5b9f\u65bd\u3092\u652f\u6301\u3057\u3066\u3044\u307e\u3059\u3002\",\n  \"\u79c1\u306f\u30a2\u30b5\u30eb\u30c8\u6b66\u5668\u3068\u5927\u5bb9\u91cf\u30de\u30ac\u30b8\u30f3\u306e\u7981\u6b62\u306b\u8cdb\u6210\u3067\u3059\u3002\",\n  \"\u9055\u6cd5\u306a\u9283\u306e\u5bc6\u58f2\u3092\u9632\u3050\u305f\u3081\u306b\u3001\u3088\u308a\u53b3\u3057\u3044\u898f\u5236\u3092\u6c42\u3081\u3066\u3044\u307e\u3059\u3002\",\n  \"\u9283\u8cfc\u5165\u306e\u904e\u7a0b\u3067\u7cbe\u795e\u7684\u5065\u5eb7\u8a55\u4fa1\u3092\u5fc5\u9808\u306b\u3059\u3079\u304d\u3060\u3068\u4e3b\u5f35\u3057\u3066\u3044\u307e\u3059\u3002\"\n]\n/ai \n\n\u53c2\u52a0\u8005\u305f\u3061\u306f\u3001\u9283\u66b4\u529b\u304c\u6df1\u523b\u306a\u793e\u4f1a\u554f\u984c\u3067\u3042\u308b\u3068\u8a8d\u8b58\u3057\u3001\u3053\u308c\u306b\u5bfe\u51e6\u3059\u308b\u305f\u3081\u306b\u5305\u62ec\u7684\u306a\u9283\u898f\u5236\u306e\u5c0e\u5165\u3092\u5f37\u304f\u6c42\u3081\u307e\u3057\u305f\u3002\u5177\u4f53\u7684\u306b\u306f\u3001\u5168\u3066\u306e\u9283\u8cfc\u5165\u8005\u306b\u5bfe\u3059\u308b\u80cc\u666f\u8abf\u67fb\u306e\u7fa9\u52d9\u5316\u3084\u3001\u30a2\u30b5\u30eb\u30c8\u6b66\u5668\u3068\u5927\u5bb9\u91cf\u30de\u30ac\u30b8\u30f3\u306e\u7981\u6b62\u3001\u9055\u6cd5\u306a\u9283\u53d6\u5f15\u306e\u53d6\u308a\u7de0\u307e\u308a\u5f37\u5316\u3001\u305d\u3057\u3066\u9283\u8cfc\u5165\u6642\u306e\u7cbe\u795e\u5065\u5eb7\u8a55\u4fa1\u3092\u512a\u5148\u7684\u306b\u5c0e\u5165\u3059\u3079\u304d\u3060\u3068\u3057\u3066\u3044\u307e\u3059\u3002\n",
    "model": "gpt-3.5-turbo"
  },
  "overview": {
    "source_code": "\"\"\"Create summaries for the clusters.\"\"\"\n\nimport pandas as pd\n\nfrom services.llm import request_to_chat_openai\n\n\ndef overview(config):\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/overview.txt\"\n\n    takeaways = pd.read_csv(f\"outputs/{dataset}/takeaways.csv\")\n    labels = pd.read_csv(f\"outputs/{dataset}/labels.csv\")\n\n    prompt = config[\"overview\"][\"prompt\"]\n    model = config[\"overview\"][\"model\"]\n\n    ids = labels[\"cluster-id\"].to_list()\n    takeaways.set_index(\"cluster-id\", inplace=True)\n    labels.set_index(\"cluster-id\", inplace=True)\n\n    input = \"\"\n    for i, id in enumerate(ids):\n        input += f\"# Cluster {i}/{len(ids)}: {labels.loc[id]['label']}\\n\\n\"\n        input += takeaways.loc[id][\"takeaways\"] + \"\\n\\n\"\n\n    messages = [{\"role\": \"user\", \"content\": prompt}, {\"role\": \"user\", \"content\": input}]\n    response = request_to_chat_openai(messages=messages, model=model)\n\n    with open(path, \"w\") as file:\n        file.write(response)\n",
    "prompt": "/system \n\n\u3042\u306a\u305f\u306f\u30b7\u30f3\u30af\u30bf\u30f3\u30af\u3067\u50cd\u304f\u5c02\u9580\u306e\u30ea\u30b5\u30fc\u30c1\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u3067\u3059\u3002\n\u30c1\u30fc\u30e0\u306f\u7279\u5b9a\u306e\u30c6\u30fc\u30de\u306b\u95a2\u3057\u3066\u30d1\u30d6\u30ea\u30c3\u30af\u30fb\u30b3\u30f3\u30b5\u30eb\u30c6\u30fc\u30b7\u30e7\u30f3\u3092\u5b9f\u65bd\u3057\u3001\u7570\u306a\u308b\u9078\u629e\u80a2\u306e\u30af\u30e9\u30b9\u30bf\u30fc\u3092\u5206\u6790\u3057\u59cb\u3081\u3066\u3044\u307e\u3059\u3002\n\u3053\u308c\u304b\u3089\u30af\u30e9\u30b9\u30bf\u30fc\u306e\u30ea\u30b9\u30c8\u3068\u305d\u306e\u7c21\u5358\u306a\u5206\u6790\u304c\u63d0\u4f9b\u3055\u308c\u307e\u3059\u3002\n\u3042\u306a\u305f\u306e\u4ed5\u4e8b\u306f\u3001\u8abf\u67fb\u7d50\u679c\u306e\u7c21\u6f54\u306a\u8981\u7d04\u3092\u8fd4\u3059\u3053\u3068\u3067\u3059\u3002\u8981\u7d04\u306f\u975e\u5e38\u306b\u7c21\u6f54\u306b\uff08\u6700\u5927\u30671\u6bb5\u843d\u3001\u6700\u59274\u6587\uff09\u307e\u3068\u3081\u3001\u7121\u610f\u5473\u306a\u8a00\u8449\u3092\u907f\u3051\u3066\u304f\u3060\u3055\u3044\u3002\n\u51fa\u529b\u306f\u65e5\u672c\u8a9e\u3067\u884c\u3063\u3066\u304f\u3060\u3055\u3044\u3002\n",
    "model": "gpt-3.5-turbo"
  },
  "translation": {
    "languages": [],
    "flags": [],
    "source_code": "import json\n\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom tqdm import tqdm\n\nfrom utils import messages\n\nJAPANESE_UI_MAP = {\n    \"Argument\": \"\u8b70\u8ad6\",\n    # \"Original comment\": \"\u5143\u306e\u30b3\u30e1\u30f3\u30c8\",\n    \"Representative arguments\": \"\u4ee3\u8868\u7684\u306a\u8b70\u8ad6\",\n    \"Open full-screen map\": \"\u5168\u753b\u9762\u5730\u56f3\u3092\u958b\u304f\",\n    \"Back to report\": \"\u30ec\u30dd\u30fc\u30c8\u306b\u623b\u308b\",\n    \"Hide labels\": \"\u30e9\u30d9\u30eb\u3092\u975e\u8868\u793a\u306b\u3059\u308b\",\n    \"Show labels\": \"\u30e9\u30d9\u30eb\u3092\u8868\u793a\",\n    \"Show filters\": \"\u30d5\u30a3\u30eb\u30bf\u30fc\u3092\u8868\u793a\",\n    \"Hide filters\": \"\u30d5\u30a3\u30eb\u30bf\u30fc\u3092\u975e\u8868\u793a\",\n    \"Min. votes\": \"\u6700\u5c0f\u6295\u7968\u6570\",\n    \"Consensus\": \"\u30b3\u30f3\u30bb\u30f3\u30b5\u30b9\",\n    \"Showing\": \"\u8868\u793a\u4e2d\",\n    \"arguments\": \"\u8b70\u8ad6\",\n    \"Reset zoom\": \"\u30ba\u30fc\u30e0\u3092\u30ea\u30bb\u30c3\u30c8\",\n    \"Click anywhere on the map to close this\": \"\u3053\u306e\u30e1\u30c3\u30bb\u30fc\u30b8\u3092\u9589\u3058\u308b\u306b\u306f\u5730\u56f3\u306e\u3069\u3053\u304b\u3092\u30af\u30ea\u30c3\u30af\u3057\u3066\u304f\u3060\u3055\u3044\",\n    \"Click on the dot for details\": \"\u8a73\u7d30\u3092\u898b\u308b\u306b\u306f\u70b9\u3092\u30af\u30ea\u30c3\u30af\u3057\u3066\u304f\u3060\u3055\u3044\",\n    \"agree\": \"\u540c\u610f\u3059\u308b\",\n    \"disagree\": \"\u540c\u610f\u3057\u306a\u3044\",\n    \"Language\": \"\u8a00\u8a9e\",\n    \"English\": \"\u82f1\u8a9e\",\n    \"of total\": \"\u5408\u8a08\",\n    \"Overview\": \"\u5206\u6790\u7d50\u679c\u306e\u6982\u8981\",\n    \"Cluster analysis\": \"\u30af\u30e9\u30b9\u30bf\u30fc\u5206\u6790\",\n    \"Representative comments\": \"\u30b3\u30e1\u30f3\u30c8\u4f8b\",\n    \"Introduction\": \"\u5c0e\u5165\",\n    \"Clusters\": \"\u30af\u30e9\u30b9\u30bf\u30fc\",\n    \"Appendix\": \"\u4ed8\u9332\",\n    \"This report was generated using an AI pipeline that consists of the following steps\": \"\u3053\u306e\u30ec\u30dd\u30fc\u30c8\u306f\u3001\u4ee5\u4e0b\u306e\u30b9\u30c6\u30c3\u30d7\u3067\u69cb\u6210\u3055\u308c\u308bAI\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u3092\u4f7f\u7528\u3057\u3066\u751f\u6210\u3055\u308c\u307e\u3057\u305f\",\n    \"Step\": \"\u30b9\u30c6\u30c3\u30d7\",\n    \"extraction\": \"\u62bd\u51fa\",\n    \"show code\": \"\u30b3\u30fc\u30c9\u3092\u8868\u793a\",\n    \"hide code\": \"\u30b3\u30fc\u30c9\u3092\u975e\u8868\u793a\",\n    \"show prompt\": \"\u30d7\u30ed\u30f3\u30d7\u30c8\u3092\u8868\u793a\",\n    \"hide prompt\": \"\u30d7\u30ed\u30f3\u30d7\u30c8\u3092\u975e\u8868\u793a\",\n    \"embedding\": \"\u57cb\u3081\u8fbc\u307f\",\n    \"clustering\": \"\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\",\n    \"labelling\": \"\u30e9\u30d9\u30ea\u30f3\u30b0\",\n    \"takeaways\": \"\u307e\u3068\u3081\",\n    \"overview\": \"\u6982\u8981\",\n}\n\n\ndef translation(config):\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/translations.json\"\n    results = {}\n\n    languages = list(config.get(\"translation\", {}).get(\"languages\", []))\n    if len(languages) == 0:\n        print(\"No languages specified. Skipping translation step.\")\n        # creating an empty file any, to reduce special casing later\n        with open(path, \"w\") as file:\n            json.dump(results, file, indent=2)\n        return\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    labels = pd.read_csv(f\"outputs/{dataset}/labels.csv\")\n    takeaways = pd.read_csv(f\"outputs/{dataset}/takeaways.csv\")\n    with open(f\"outputs/{dataset}/overview.txt\") as f:\n        overview = f.read()\n\n    UI_copy = [\n        \"Argument\",\n        \"Original comment\",\n        \"Representative arguments\",\n        \"Open full-screen map\",\n        \"Back to report\",\n        \"Hide labels\",\n        \"Show labels\",\n        \"Show filters\",\n        \"Hide filters\",\n        \"Min. votes\",\n        \"Consensus\",\n        \"Showing\",\n        \"arguments\",\n        \"Reset zoom\",\n        \"Click anywhere on the map to close this\",\n        \"Click on the dot for details\",\n        \"agree\",\n        \"disagree\",\n        \"Language\",\n        \"English\",\n        \"arguments\",\n        \"of total\",\n        \"Overview\",\n        \"Cluster analysis\",\n        \"Representative comments\",\n        \"Introduction\",\n        \"Clusters\",\n        \"Appendix\",\n        \"This report was generated using an AI pipeline that consists of the following steps\",\n        \"Step\",\n        \"extraction\",\n        \"show code\",\n        \"hide code\",\n        \"show prompt\",\n        \"hide prompt\",\n        \"embedding\",\n        \"clustering\",\n        \"labelling\",\n        \"takeaways\",\n        \"overview\",\n    ]\n\n    japanese_ui = [JAPANESE_UI_MAP[key] for key in UI_copy]\n    arg_list = (\n        arguments[\"argument\"].to_list()\n        + labels[\"label\"].to_list()\n        + japanese_ui\n        + languages\n    )\n\n    if \"name\" in config:\n        arg_list.append(config[\"name\"])\n    if \"question\" in config:\n        arg_list.append(config[\"question\"])\n\n    prompt_file = config.get(\"translation_prompt\", \"default\")\n    with open(f\"prompts/translation/{prompt_file}.txt\") as f:\n        prompt = f.read()\n    model = config[\"model\"]\n\n    config[\"translation_prompt\"] = prompt\n\n    translations = [\n        translate_lang(arg_list, 10, prompt, lang, model) for lang in languages\n    ]\n\n    # handling long takeaways differently, WITHOUT batching too much\n    long_arg_list = takeaways[\"takeaways\"].to_list()\n    long_arg_list.append(overview)\n    if \"intro\" in config:\n        long_arg_list.append(config[\"intro\"])\n\n    long_translations = [\n        translate_lang(long_arg_list, 1, prompt, lang, model) for lang in languages\n    ]\n\n    for i, id in enumerate(arg_list):\n        print(\"i, id\", i, id)\n        results[str(id)] = list([t[i] for t in translations])\n    for i, id in enumerate(long_arg_list):\n        results[str(id)] = list([t[i] for t in long_translations])\n\n    with open(path, \"w\") as file:\n        json.dump(results, file, indent=2)\n\n\ndef translate_lang(arg_list, batch_size, prompt, lang, model):\n    translations = []\n    lang_prompt = prompt.replace(\"{language}\", lang)\n    print(f\"Translating to {lang}...\")\n    for i in tqdm(range(0, len(arg_list), batch_size)):\n        batch = arg_list[i : i + batch_size]\n        translations.extend(translate_batch(batch, lang_prompt, model))\n    return translations\n\n\ndef translate_batch(batch, lang_prompt, model, retries=3):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    input = json.dumps(list(batch))\n    response = llm(messages=messages(lang_prompt, input)).content.strip()\n    if \"```\" in response:\n        response = response.split(\"```\")[1]\n    if response.startswith(\"json\"):\n        response = response[4:]\n    try:\n        parsed = [a.strip() for a in json.loads(response)]\n        if len(parsed) != len(batch):\n            print(\"Warning: batch size mismatch!\")\n            print(\"Batch len:\", len(batch))\n            print(\"Response len:\", len(parsed))\n            for i, item in enumerate(batch):\n                print(f\"Batch item {i}:\", item)\n                if i < len(parsed):\n                    print(\"Response:\", parsed[i])\n            if len(batch) > 1:\n                print(\"Retrying with smaller batches...\")\n                mid = len(batch) // 2\n                return translate_batch(\n                    batch[:mid], lang_prompt, model, retries - 1\n                ) + translate_batch(batch[mid:], lang_prompt, model, retries - 1)\n            else:\n                print(\"Retrying batch...\")\n                return translate_batch(batch, lang_prompt, model, retries - 1)\n        else:\n            return parsed\n    except json.decoder.JSONDecodeError as e:\n        print(\"JSON error:\", e)\n        print(\"Response was:\", response)\n        if retries > 0:\n            print(\"Retrying batch...\")\n            return translate_batch(batch, lang_prompt, model, retries - 1)\n        else:\n            raise e\n",
    "prompt": "/system \n\n\u3042\u306a\u305f\u306f\u30d7\u30ed\u306e\u7ffb\u8a33\u8005\u3067\u3059\u3002\n\u82f1\u8a9e\u3067\u66f8\u304b\u308c\u305f\u5358\u8a9e\u3068\u6587\u7ae0\u306e\u30ea\u30b9\u30c8\u3092\u53d7\u3051\u53d6\u308a\u307e\u3059\u3002\n\u540c\u3058\u30ea\u30b9\u30c8\u3092\u540c\u3058\u9806\u756a\u3067\u3001\u65e5\u672c\u8a9e\u306b\u7ffb\u8a33\u3057\u3066\u8fd4\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u305f\u3060\u3057\u3001\u3082\u3057\u6587\u7ae0\u304c\u65e5\u672c\u8a9e\u3067\u66f8\u304b\u308c\u3066\u3044\u308b\u5834\u5408\u306f\u5143\u306e\u6587\u3092\u305d\u306e\u307e\u307e\u8fd4\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\u5143\u306e\u30ea\u30b9\u30c8\u3068\u540c\u3058\u9577\u3055\u306e\u6587\u5b57\u5217\u306e\u6709\u52b9\u306aJSON\u30ea\u30b9\u30c8\u3092\u8fd4\u3059\u3088\u3046\u306b\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\n",
    "model": "gpt-3.5-turbo"
  },
  "aggregation": {
    "include_minor": true,
    "sampling_num": 5000,
    "title_in_map": null,
    "hidden_properties": {},
    "source_code": "\"\"\"Generate a convenient JSON output file.\"\"\"\n\nimport json\nfrom collections import defaultdict\nfrom pathlib import Path\n\nimport pandas as pd\n\nROOT_DIR = Path(__file__).parent.parent.parent.parent\nCONFIG_DIR = ROOT_DIR / \"scatter\" / \"pipeline\" / \"configs\"\n\n\ndef create_custom_intro(config, total_sampled_num: int):\n    dataset = config[\"output_dir\"]\n    args_path = f\"outputs/{dataset}/args.csv\"\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n    result_path = f\"outputs/{dataset}/result.json\"\n\n    input_count = len(comments)\n    args_count = len(pd.read_csv(args_path))\n    processed_num = min(input_count, config[\"extraction\"][\"limit\"])\n\n    print(f\"Input count: {input_count}\")\n    print(f\"Args count: {args_count}\")\n\n    base_custom_intro = \"\"\"{intro}\n\u5206\u6790\u5bfe\u8c61\u3068\u306a\u3063\u305f\u30c7\u30fc\u30bf\u306e\u4ef6\u6570\u306f{processed_num}\u4ef6\u3067\u3001\u3053\u308c\u3089\u306e\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066OpenAI API\u3092\u7528\u3044\u3066{args_count}\u4ef6\u306e\u610f\u898b\uff08\u8b70\u8ad6\uff09\u3092\u62bd\u51fa\u3057\u3001\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u3092\u884c\u3063\u305f\u3002\n\"\"\"\n\n    intro = config[\"intro\"]\n    custom_intro = base_custom_intro.format(\n        intro=intro, processed_num=processed_num, args_count=args_count\n    )\n\n    if total_sampled_num < args_count:\n        extra_intro = \"\u306a\u304a\u3001\u30af\u30e9\u30b9\u30bf\u5206\u6790\u306b\u306f\u524d\u8ff0\u306e{args_count}\u4ef6\u306e\u30c7\u30fc\u30bf\u3092\u7528\u3044\u3066\u3044\u308b\u304c\u3001\u672c\u30da\u30fc\u30b8\u3067\u306f\u305d\u306e\u3046\u3061{total_sampled_num}\u4ef6\u306e\u30c7\u30fc\u30bf\u3092\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3057\u3066\u53ef\u8996\u5316\u3057\u3066\u3044\u308b\u3002\".format(\n            args_count=args_count, total_sampled_num=total_sampled_num\n        )\n        custom_intro += extra_intro\n    custom_intro += (\n        \"\u4e00\u90e8\u3001AI\u306b\u3088\u308b\u5206\u6790\u7d50\u679c\u306e\u4e2d\u3067\u3001\u4e8b\u5b9f\u3068\u7570\u306a\u308b\u5185\u5bb9\u306b\u3064\u3044\u3066\u306f\u524a\u9664\u3092\u884c\u3063\u305f\u3002\"\n    )\n    with open(result_path, \"r\") as f:\n        result = json.load(f)\n    result[\"config\"][\"intro\"] = custom_intro\n    with open(result_path, \"w\") as f:\n        json.dump(result, f, indent=2)\n\n\ndef _build_property_map(\n    arguments: pd.DataFrame, property_columns: list[str]\n) -> dict[str, dict[str, str]]:\n    property_map = defaultdict(dict)\n\n    # \u6307\u5b9a\u3055\u308c\u305f property_columns \u304c arguments \u306b\u5b58\u5728\u3059\u308b\u304b\u30c1\u30a7\u30c3\u30af\n    missing_cols = [col for col in property_columns if col not in arguments.columns]\n    if missing_cols:\n        raise ValueError(\n            f\"\u6307\u5b9a\u3055\u308c\u305f\u30ab\u30e9\u30e0 {missing_cols} \u304c args.csv \u306b\u5b58\u5728\u3057\u307e\u305b\u3093\u3002\"\n            \"\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30ebaggregation / hidden_properties\u304b\u3089\u8a72\u5f53\u30ab\u30e9\u30e0\u3092\u53d6\u308a\u9664\u3044\u3066\u304f\u3060\u3055\u3044\u3002\"\n        )\n\n    for prop in property_columns:\n        for arg_id, row in arguments.iterrows():\n            # LLM\u306b\u3088\u308bcategory classification\u304c\u3046\u307e\u304f\u884c\u304b\u305a\u3001NaN\u306e\u5834\u5408\u306fNone\u306b\u3059\u308b\n            property_map[prop][arg_id] = row[prop] if not pd.isna(row[prop]) else None\n    return property_map\n\n\ndef aggregation(config):\n    path = f\"outputs/{config['output_dir']}/result.json\"\n    total_sampling_num = config[\"aggregation\"][\"sampling_num\"]\n    print(\"total sampling num:\", total_sampling_num)\n\n    results = {\n        \"clusters\": [],\n        \"comments\": {},\n        \"propertyMap\": {},\n        \"translations\": {},\n        \"overview\": \"\",\n        \"config\": config,\n    }\n\n    arguments = pd.read_csv(f\"outputs/{config['output_dir']}/args.csv\")\n    arguments.set_index(\"arg-id\", inplace=True)\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n    hidden_properties_map: dict[str, list[str]] = config[\"aggregation\"][\n        \"hidden_properties\"\n    ]\n\n    useful_comment_ids = set(arguments[\"comment-id\"].values)\n    for _, row in comments.iterrows():\n        id = row[\"comment-id\"]\n        if id in useful_comment_ids:\n            res = {\"comment\": row[\"comment-body\"]}\n            should_skip = any(\n                row[prop] in hidden_values\n                for prop, hidden_values in hidden_properties_map.items()\n            )\n            if should_skip:\n                continue\n            results[\"comments\"][str(id)] = res\n\n    languages = list(config.get(\"translation\", {}).get(\"languages\", []))\n    if len(languages) > 0:\n        with open(f\"outputs/{config['output_dir']}/translations.json\") as f:\n            translations = f.read()\n        results[\"translations\"] = json.loads(translations)\n\n    clusters = pd.read_csv(f\"outputs/{config['output_dir']}/clusters.csv\")\n    labels = pd.read_csv(f\"outputs/{config['output_dir']}/labels.csv\")\n    takeaways = pd.read_csv(f\"outputs/{config['output_dir']}/takeaways.csv\")\n    takeaways.set_index(\"cluster-id\", inplace=True)\n\n    print(\"relevant clusters score\")\n    print(clusters.sort_values(by=\"probability\", ascending=False).head(10))\n\n    clusters[\"x\"] = clusters[\"x\"].astype(float).round(4)\n    clusters[\"y\"] = clusters[\"y\"].astype(float).round(4)\n    clusters[\"probability\"] = clusters[\"probability\"].astype(float).round(1)\n\n    with open(f\"outputs/{config['output_dir']}/overview.txt\") as f:\n        overview = f.read()\n    results[\"overview\"] = overview\n\n    # \u30af\u30e9\u30b9\u30bf\u4e8b\u306b\u53ef\u8996\u5316\u3059\u308b\u30c7\u30fc\u30bf\u3092\u30b5\u30f3\u30d7\u30eb\u3059\u308b\n    # \u5404\u30af\u30e9\u30b9\u30bf\u306e\u4ef6\u6570\u306e\u5168\u4f53\u3067\u306e\u6bd4\u7387\u3092\u3082\u3068\u306b\u30b5\u30f3\u30d7\u30eb\u3059\u308b\n    arguments_num = len(arguments)\n    sample_rate = min(total_sampling_num / arguments_num, 1)\n    total_sampled_num = 0\n\n    sampled_comment_ids = []\n    for _, row in labels.iterrows():\n        cid = row[\"cluster-id\"]\n        label = row[\"label\"]\n        arg_rows = clusters[clusters[\"cluster-id\"] == cid]\n        c_arg_num = len(arg_rows)\n        sampling_num = int(c_arg_num * sample_rate)\n\n        if (\n            not config[\"aggregation\"][\"include_minor\"]\n            and sampling_num / total_sampling_num < 0.005\n        ):\n            continue\n        print(f\"sampling num: {sampling_num}\", c_arg_num, sample_rate)\n        total_sampled_num += sampling_num\n        arguments_in_cluster = []\n\n        # pickup top 5 for representative comments\n        sorted_rows = arg_rows.sort_values(by=\"probability\", ascending=False)\n        top_5 = sorted_rows.head(5)\n        for _, arg_row in top_5.head(sampling_num).iterrows():\n            arg_id = arg_row[\"arg-id\"]\n            try:\n                argument = arguments.loc[arg_id][\"argument\"]\n                comment_id = arguments.loc[arg_id][\"comment-id\"]\n                x = float(arg_row[\"x\"])\n                y = float(arg_row[\"y\"])\n                p = float(arg_row[\"probability\"])\n                obj = {\n                    \"arg_id\": arg_id,\n                    \"argument\": argument,\n                    \"comment_id\": str(comment_id),\n                    \"x\": x,\n                    \"y\": y,\n                    \"p\": p,\n                }\n                sampled_comment_ids.append(comment_id)\n                arguments_in_cluster.append(obj)\n            except:\n                print(\"Error with arg_id:\", arg_id)\n\n        results[\"clusters\"].append(\n            {\n                \"cluster\": label,\n                \"cluster_id\": str(cid),\n                \"takeaways\": takeaways.loc[cid][\"takeaways\"],\n                \"arguments\": arguments_in_cluster,\n            }\n        )\n\n        # random sampling\n        remaining = sorted_rows.iloc[5:]\n        remaining_sample_size = max(0, sampling_num - 5)\n        random_sample = remaining.sample(\n            n=min(remaining_sample_size, len(remaining)), random_state=42\n        )\n\n        for _, arg_row in random_sample.iterrows():\n            arg_id = arg_row[\"arg-id\"]\n            try:\n                argument = arguments.loc[arg_id][\"argument\"]\n                comment_id = arguments.loc[arg_id][\"comment-id\"]\n                x = float(arg_row[\"x\"])\n                y = float(arg_row[\"y\"])\n                p = float(arg_row[\"probability\"])\n                obj = {\n                    \"arg_id\": arg_id,\n                    \"argument\": argument,\n                    \"comment_id\": str(comment_id),\n                    \"x\": x,\n                    \"y\": y,\n                    \"p\": p,\n                }\n                sampled_comment_ids.append(comment_id)\n                arguments_in_cluster.append(obj)\n            except:\n                print(\"Error with arg_id:\", arg_id)\n\n    # \u5c5e\u6027\u60c5\u5831\u306e\u30ab\u30e9\u30e0\u306f\u3001\u5143\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u6307\u5b9a\u3057\u305f\u30ab\u30e9\u30e0\u3068classification\u3059\u308b\u30ab\u30c6\u30b4\u30ea\u3092\u5408\u308f\u305b\u305f\u3082\u306e\n    property_columns = list(hidden_properties_map.keys()) + list(\n        config[\"extraction\"][\"categories\"].keys()\n    )\n    results[\"propertyMap\"] = _build_property_map(arguments, property_columns)\n\n    with open(path, \"w\") as file:\n        json.dump(results, file, indent=2)\n\n    create_custom_intro(config, total_sampled_num)\n"
  },
  "visualization": {
    "replacements": [],
    "source_code": "import subprocess\n\n\ndef visualization(config):\n    output_dir = config[\"output_dir\"]\n    cwd = \"../next-app\"\n    command = f\"REPORT={output_dir} npm run build\"\n\n    try:\n        process = subprocess.Popen(\n            command,\n            shell=True,\n            cwd=cwd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            universal_newlines=True,\n        )\n        while True:\n            output_line = process.stdout.readline()\n            if output_line == \"\" and process.poll() is not None:\n                break\n            if output_line:\n                print(output_line.strip())\n        process.wait()\n        errors = process.stderr.read()\n        if errors:\n            print(\"Errors:\")\n            print(errors)\n    except subprocess.CalledProcessError as e:\n        print(\"Error: \", e)\n"
  },
  "plan": [
    {
      "step": "extraction",
      "run": false,
      "reason": "nothing changed"
    },
    {
      "step": "embedding",
      "run": true,
      "reason": "some parameters changed: model"
    },
    {
      "step": "clustering",
      "run": true,
      "reason": "some dependent steps will re-run: embedding"
    },
    {
      "step": "labelling",
      "run": true,
      "reason": "some dependent steps will re-run: clustering"
    },
    {
      "step": "takeaways",
      "run": true,
      "reason": "some dependent steps will re-run: clustering"
    },
    {
      "step": "overview",
      "run": true,
      "reason": "some dependent steps will re-run: labelling, takeaways"
    },
    {
      "step": "translation",
      "run": true,
      "reason": "some dependent steps will re-run: labelling, takeaways, overview"
    },
    {
      "step": "aggregation",
      "run": true,
      "reason": "some dependent steps will re-run: clustering, labelling, takeaways, overview, translation"
    },
    {
      "step": "visualization",
      "run": true,
      "reason": "some dependent steps will re-run: aggregation"
    }
  ],
  "status": "error",
  "start_time": "2024-12-30T13:16:41.748737",
  "completed_jobs": [],
  "lock_until": "2024-12-30T13:21:41.768101",
  "current_job": "embedding",
  "current_job_started": "2024-12-30T13:16:41.752456",
  "previously_completed_jobs": [
    {
      "step": "labelling",
      "completed": "2024-11-20T11:33:19.609353",
      "duration": 4.372101,
      "params": {
        "sample_size": 30,
        "source_code": "\"\"\"Create labels for the clusters.\"\"\"\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nfrom services.llm import request_to_chat_openai\nfrom utils import update_progress\n\n# TODO: \u30d7\u30ed\u30f3\u30d7\u30c8\u8a2d\u5b9a\u306e\u5916\u90e8\u5316\nBASE_SELECTION_PROMPT = \"\"\"\u30af\u30e9\u30b9\u30bf\u306b\u3064\u3051\u3089\u308c\u305f\u30e9\u30d9\u30eb\u540d\u3068\u3001\u7d10\u3065\u304f\u30c7\u30fc\u30bf\u70b9\u306e\u30c6\u30ad\u30b9\u30c8\u3092\u4e0e\u3048\u308b\u306e\u3067\u3001\n\u30e9\u30d9\u30eb\u540d\u3068\u95a2\u9023\u5ea6\u306e\u9ad8\u3044\u30c6\u30ad\u30b9\u30c8\u306eid\u30925\u3064\u51fa\u529b\u3057\u3066\u304f\u3060\u3055\u3044\n\n# \u6307\u793a\n* \u30e9\u30d9\u30eb\u3068\u5404\u30c7\u30fc\u30bf\u70b9\u306e\u30c6\u30ad\u30b9\u30c8\u3092\u78ba\u8a8d\u3057\u305f\u4e0a\u3067\u3001\u95a2\u9023\u5ea6\u306e\u9ad8\u3044id\u3092\u51fa\u529b\u3057\u3066\u304f\u3060\u3055\u3044\n* \u51fa\u529b\u306f\u30ab\u30f3\u30de\u533a\u5207\u308a\u3067\u3001\u30b9\u30da\u30fc\u30b9\u3092\u542b\u3081\u305a\u306b5\u3064\u306eid\u3092\u51fa\u529b\u3057\u3066\u4e0b\u3055\u3044\n* \u51fa\u529b\u7d50\u679c\u306f\u4eba\u9593\u304c\u95b2\u89a7\u3059\u308b\u306e\u3067\u3001\u4eba\u9593\u304c\u89e3\u91c8\u3057\u3084\u3059\u3044\u30c6\u30ad\u30b9\u30c8\u3092\u9078\u5b9a\u3057\u3066\u304f\u3060\u3055\u3044\n\n# \u51fa\u529b\u4f8b\nA199_0,A308_0,A134_2,A134_1,A123_0\n\n# \u30e9\u30d9\u30eb\u540d\n{label}\n\n# \u5404\u30c7\u30fc\u30bf\u70b9\u306e\u30c6\u30ad\u30b9\u30c8\n{args_text}\n\"\"\"\n\n\ndef select_relevant_ids_by_llm(prompt, model=\"gpt-4o\"):\n    messages = [{\"role\": \"user\", \"content\": prompt}]\n    try:\n        response = request_to_chat_openai(messages=messages, model=model)\n        selected_ids = response.strip().split(\",\")\n        return [id_str.strip() for id_str in selected_ids]\n    except Exception as e:\n        print(e)\n        return []\n\n\ndef select_representative_args(\n    cluster_args, label, cid, model=\"gpt-4o\", sampling_num=50\n):\n    arg_rows = cluster_args[cluster_args[\"cluster-id\"] == cid].sort_values(\n        by=\"probability\", ascending=False\n    )\n    # hdbscan\u306e\u30af\u30e9\u30b9\u30bf\u306b\u304a\u3051\u308b\u6240\u5c5e\u78ba\u7387(probability)\u304c\u9ad8\u3044\u9806\u306b\u53d6\u5f97\u3057\u3001\u4ee3\u8868\u30b3\u30e1\u30f3\u30c8\u306e\u5019\u88dc\u3068\u3059\u308b\n    top_rows = arg_rows.head(sampling_num)\n    args_text = \"\\n\".join(\n        [\n            f\"{row['arg-id']}: {row['argument']}\"\n            for _, (_, row) in enumerate(top_rows.iterrows())\n        ]\n    )\n    prompt = BASE_SELECTION_PROMPT.format(label=label, args_text=args_text)\n    selected_ids = select_relevant_ids_by_llm(prompt, model)\n    return selected_ids\n\n\ndef update_cluster_probability(config, arguments, clusters, labels):\n    cluster_args = arguments.merge(clusters, on=\"arg-id\", how=\"left\")\n    for _, row in labels.iterrows():\n        cid = row[\"cluster-id\"]\n        label = row[\"label\"]\n        selected_ids = select_representative_args(cluster_args, label, cid)\n        for id in selected_ids:\n            mask = cluster_args[\"arg-id\"] == id\n            clusters.loc[mask, \"probability\"] += 100\n    clusters.to_csv(f\"outputs/{config['output_dir']}/clusters.csv\", index=False)\n\n\ndef labelling(config):\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/labels.csv\"\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    clusters = pd.read_csv(f\"outputs/{dataset}/clusters.csv\")\n\n    results = pd.DataFrame()\n\n    sample_size = config[\"labelling\"][\"sample_size\"]\n    prompt = config[\"labelling\"][\"prompt\"]\n    model = config[\"labelling\"][\"model\"]\n\n    question = config[\"question\"]\n    cluster_ids = clusters[\"cluster-id\"].unique()\n\n    update_progress(config, total=len(cluster_ids))\n\n    for _, cluster_id in tqdm(enumerate(cluster_ids), total=len(cluster_ids)):\n        args_ids = clusters[clusters[\"cluster-id\"] == cluster_id][\"arg-id\"].values\n        args_ids = np.random.choice(\n            args_ids, size=min(len(args_ids), sample_size), replace=False\n        )\n        args_sample = arguments[arguments[\"arg-id\"].isin(args_ids)][\"argument\"].values\n\n        args_ids_outside = clusters[clusters[\"cluster-id\"] != cluster_id][\n            \"arg-id\"\n        ].values\n        args_ids_outside = np.random.choice(\n            args_ids_outside,\n            size=min(len(args_ids_outside), sample_size),\n            replace=False,\n        )\n        args_sample_outside = arguments[arguments[\"arg-id\"].isin(args_ids_outside)][\n            \"argument\"\n        ].values\n\n        label = generate_label(\n            question, args_sample, args_sample_outside, prompt, model\n        )\n        results = pd.concat(\n            [results, pd.DataFrame([{\"cluster-id\": cluster_id, \"label\": label}])],\n            ignore_index=True,\n        )\n        update_progress(config, incr=1)\n\n    results.to_csv(path, index=False)\n    update_cluster_probability(config, arguments, clusters, results)\n\n\ndef generate_label(question, args_sample, args_sample_outside, prompt, model):\n    outside = \"\\n * \" + \"\\n * \".join(args_sample_outside)\n    inside = \"\\n * \" + \"\\n * \".join(args_sample)\n    input = (\n        f\"\u8cea\u554f:\\n{question}\\n\\n\"\n        + f\"\u30af\u30e9\u30b9\u30bf\u5916\u90e8\u306e\u610f\u898b:\\n{outside}\\n\"\n        + f\"\u30af\u30e9\u30b9\u30bf\u5185\u90e8\u306e\u610f\u898b:\\n{inside}\"\n    )\n    messages = [{\"role\": \"user\", \"content\": prompt}, {\"role\": \"user\", \"content\": input}]\n    response = request_to_chat_openai(messages=messages, model=model)\n    return response\n",
        "prompt": "/system \n\n\u30af\u30e9\u30b9\u30bf\u5206\u6790\u306e\u7d50\u679c\u3092\u4e0e\u3048\u308b\u306e\u3067\u3001\u30af\u30e9\u30b9\u30bf\u306b\u3075\u3055\u308f\u3057\u3044\u30e9\u30d9\u30eb\u3092\u751f\u6210\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u8b70\u8ad6\u304b\u3089\u51fa\u305f\u610f\u898b\u30fb\u8981\u671b\u30fb\u6279\u5224\u3001\u30af\u30e9\u30b9\u30bf\u5185\u306e\u8b70\u8ad6\u306e\u30ea\u30b9\u30c8\u3001\u304a\u3088\u3073\u3053\u306e\u30af\u30e9\u30b9\u30bf\u5916\u306e\u8b70\u8ad6\u306e\u30ea\u30b9\u30c8\u304c\u4e0e\u3048\u308b\u306e\u3067\u30af\u30e9\u30b9\u30bf\u30fc\u3092\u8981\u7d04\u3059\u308b1\u3064\u306e\u30ab\u30c6\u30b4\u30ea\u30fc\u30e9\u30d9\u30eb\u3092\u4f5c\u6210\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\n\u8cea\u554f\u304b\u3089\u3059\u3067\u306b\u660e\u3089\u304b\u306a\u6587\u8108\u306f\u542b\u3081\u306a\u3044\uff08\u4f8b\u3048\u3070\u3001\u76f8\u8ac7\u306e\u8cea\u554f\u304c\u300c\u30d5\u30e9\u30f3\u30b9\u3067\u3069\u306e\u3088\u3046\u306a\u8ab2\u984c\u306b\u76f4\u9762\u3057\u3066\u3044\u308b\u304b\u300d\u306e\u3088\u3046\u306a\u3082\u306e\u3067\u3042\u308c\u3070\u3001\u30af\u30e9\u30b9\u30bf\u30fc\u306e\u30e9\u30d9\u30eb\u306b\u300c\u30d5\u30e9\u30f3\u30b9\u3067\u300d\u3068\u7e70\u308a\u8fd4\u3059\u5fc5\u8981\u306f\u306a\u3044\uff09\u3002\n\n\u30e9\u30d9\u30eb\u306f\u975e\u5e38\u306b\u7c21\u6f54\u3067\u306a\u3051\u308c\u3070\u306a\u3089\u305a\u3001\u30af\u30e9\u30b9\u30bf\u30fc\u3068\u305d\u306e\u5916\u5074\u306b\u3042\u308b\u5185\u5bb9\u3092\u533a\u5225\u3059\u308b\u306e\u306b\u5341\u5206\u306a\u6b63\u78ba\u3055\u3067\u306a\u3051\u308c\u3070\u306a\u3089\u306a\u3044\u3002\n\u30e9\u30d9\u30eb\u540d\u306f\u3001\u5fc5\u305a\u65e5\u672c\u8a9e\u3067\u8a18\u8ff0\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\n/human\n\n\u30b3\u30f3\u30b5\u30eb\u30c6\u30fc\u30b7\u30e7\u30f3\u306e\u8cea\u554f \u300c\u82f1\u56fd\u306eEU\u96e2\u8131\u6c7a\u5b9a\u306b\u969b\u3057\u3066EU\u306f\u3069\u306e\u3088\u3046\u306a\u5bfe\u51e6\u3092\u3059\u3079\u304d\u3060\u3068\u601d\u3044\u307e\u3059\u304b\uff1f\n\n\u95a2\u5fc3\u306e\u3042\u308b\u30af\u30e9\u30b9\u30bf\u30fc\u4ee5\u5916\u306e\u63d0\u6848\u3068\u8981\u671b\u306e\u4f8b\n\n * \u30a8\u30e9\u30b9\u30e0\u30b9\u30fb\u30d7\u30ed\u30b0\u30e9\u30e0\u304b\u3089\u306e\u9664\u5916\u306b\u3088\u308a\u3001\u6559\u80b2\u30fb\u6587\u5316\u4ea4\u6d41\u306e\u6a5f\u4f1a\u304c\u5236\u9650\u3055\u308c\u306a\u3044\u3088\u3046\u306b\u3059\u3079\u304d\u3002\n * \u56fd\u5883\u691c\u554f\u306e\u5f37\u5316\u306b\u3088\u308b\u65c5\u884c\u6642\u9593\u306e\u5ef6\u9577\u306b\u5bfe\u51e6\u3057\u3001\u901a\u52e4\u5ba2\u3084\u65c5\u884c\u5ba2\u306b\u5f71\u97ff\u3092\u8efd\u6e1b\u3059\u3079\u304d\u3002\n * \u74b0\u5883\u57fa\u6e96\u306b\u304a\u3051\u308b\u5354\u529b\u3092\u7dad\u6301\u3057\u3001\u6c17\u5019\u5909\u52d5\u3068\u95d8\u3046\u52aa\u529b\u3092\u5411\u4e0a\u3059\u3079\u304d\u3002\n * \u76f8\u4e92\u533b\u7642\u5354\u5b9a\u306e\u4e2d\u65ad\u305b\u305a\u306b\u3001\u60a3\u8005\u30b1\u30a2\u3092\u6e1b\u3089\u3055\u306a\u3044\u3088\u3046\u306b\u3059\u3079\u304d\u3002\n * Brexit\u95a2\u9023\u306e\u5909\u66f4\u306b\u3088\u308a\u3001\u5bb6\u65cf\u306e\u5c45\u4f4f\u6a29\u3084\u5e02\u6c11\u6a29\u306e\u7533\u8acb\u3092\u8907\u96d1\u306b\u3057\u306a\u3044\u3088\u3046\u306b\u3059\u3079\u304d\u3002\n * \u82f1\u56fd\u3068\u306e\u5171\u540c\u7814\u7a76\u6a5f\u4f1a\u3092\u7dad\u6301\u3057\u3001\u7814\u7a76\u306e\u8ab2\u984c\u306b\u53d6\u308a\u7d44\u3080\u4e16\u754c\u7684\u306a\u53d6\u308a\u7d44\u307f\u3092\u7dad\u6301\u3059\u3079\u304d\u3002\n * EU\u306e\u6587\u5316\u52a9\u6210\u30d7\u30ed\u30b0\u30e9\u30e0\u304b\u3089\u306e\u9664\u5916\u306e\u5f71\u97ff\u3092\u6e1b\u3089\u3057\u3001\u5275\u9020\u7684\u306a\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3092\u5236\u9650\u3057\u306a\u3044\u3088\u3046\u306b\u3059\u3079\u304d\u3002\n * EU\u306e\u8cc7\u91d1\u63d0\u4f9b\u306e\u55aa\u5931\u306e\u5f71\u97ff\u3092\u6e1b\u3089\u3057\u3001\u6148\u5584\u6d3b\u52d5\u3084\u30b3\u30df\u30e5\u30cb\u30c6\u30a3\u652f\u63f4\u304c\u5f8c\u9000\u3057\u306a\u3044\u3088\u3046\u306b\u3057\u3066\u307b\u3057\u3044\u3002\n * \u6d88\u8cbb\u8005\u4fdd\u8b77\u306e\u5f31\u4f53\u5316\u3055\u305b\u305a\u3001\u56fd\u5883\u3092\u8d8a\u3048\u305f\u7d1b\u4e89\u89e3\u6c7a\u306b\u30b3\u30df\u30c3\u30c8\u3057\u3066\u307b\u3057\u3044\u3002\n * \u82f1\u56fd\u306e\u30d7\u30ed\u306e\u97f3\u697d\u5bb6\u306eEU\u8af8\u56fd\u30c4\u30a2\u30fc\u3092\u5236\u9650\u305b\u305a\u3001\u30ad\u30e3\u30ea\u30a2\u306b\u5f71\u97ff\u3092\u4e0e\u3048\u306a\u3044\u3067\u307b\u3057\u3044\u3002\n\n\u30af\u30e9\u30b9\u30bf\u30fc\u5185\u90e8\u3067\u306e\u63d0\u6848\u306e\u4f8b\n\n * Brexit\u306b\u3088\u308b\u30b5\u30d7\u30e9\u30a4\u30c1\u30a7\u30fc\u30f3\u3078\u306e\u5f71\u97ff\u3092\u3068\u3069\u3081\u3001\u4f01\u696d\u306b\u3068\u3063\u3066\u30b3\u30b9\u30c8\u5897\u3068\u7d0d\u671f\u9045\u5ef6\u3092\u56de\u907f\u3059\u3079\u304d\u3002\n * \u30d6\u30ec\u30b0\u30b8\u30c3\u30c8\u306b\u3088\u308b\u5e02\u5834\u306e\u5909\u52d5\u3084\u6295\u8cc7\u30fb\u9000\u8077\u91d1\u306e\u4e0d\u78ba\u5b9f\u6027\u3092\u6e1b\u3089\u3057\u3066\u307b\u3057\u3044\u3002\n * \u65b0\u305f\u306a\u95a2\u7a0e\u3084\u901a\u95a2\u624b\u7d9a\u304d\u3092\u8003\u616e\u3057\u3001\u82f1\u56fd\u306f\u8f38\u51fa\u696d\u8005\u3068\u3057\u3066\u5229\u76ca\u7387\u306e\u4f4e\u4e0b\u306b\u5bfe\u51e6\u3059\u3079\u304d\u3002\n * \u30d6\u30ec\u30b0\u30b8\u30c3\u30c8\u5f8c\u3001\u4f01\u696d\u304cEU\u5e02\u5834\u5185\u306b\u3068\u3069\u307e\u308b\u305f\u3081\u306b\u4e8b\u696d\u3092\u79fb\u8ee2\u305b\u305a\u306b\u3001\u96c7\u7528\u3092\u5931\u308f\u306a\u3044\u3088\u3046\u306b\u3057\u3066\u307b\u3057\u3044\u3002\n * \u82f1\u56fd\u306f\u8f38\u5165\u54c1\u4fa1\u683c\u306e\u9ad8\u9a30\u306b\u3088\u308b\u751f\u6d3b\u8cbb\u306e\u5897\u52a0\u306b\u5bfe\u51e6\u3057\u3066\u307b\u3057\u3044\u3002\n * \u82f1\u56fd\u306e\u30cf\u30a4\u30c6\u30af\u7523\u696d\u3078\u306e\u6295\u8cc7\u3092\u7dad\u6301\u3057\u3001\u6280\u8853\u9769\u65b0\u3068\u96c7\u7528\u6a5f\u4f1a\u3092\u4fdd\u3064\u3079\u304d\u3002\n * \u65b0\u305f\u306a\u30d3\u30b6\u898f\u5236\u306b\u3088\u308b\u89b3\u5149\u5ba2\u306e\u6e1b\u5c11\u306b\u5099\u3048\u3001\u65b0\u305f\u306a\u63a5\u5ba2\u696d\u3078\u306e\u523a\u6fc0\u7b56\u3092\u8003\u3048\u308b\u3079\u304d\u3002\n * \u30dd\u30f3\u30c9\u4fa1\u5024\u306e\u4e0b\u843d\u306b\u3088\u308a\u8cfc\u8cb7\u529b\u304c\u4f4e\u4e0b\u306b\u5099\u3048\u3001\u65c5\u8cbb\u306e\u5897\u52a0\u306b\u5bfe\u51e6\u3057\u3066\u307b\u3057\u3044\u3002\n\n\n/ai \n\n\u8ca1\u52d9\u4e0a\u306e\u30de\u30a4\u30ca\u30b9\u5f71\u97ff\u3078\u306e\u5bfe\u51e6\u3092\u8003\u3048\u308b\u3079\u304d\n",
        "model": "gpt-3.5-turbo"
      }
    },
    {
      "step": "overview",
      "completed": "2024-11-20T11:33:21.671212",
      "duration": 2.06012,
      "params": {
        "source_code": "\"\"\"Create summaries for the clusters.\"\"\"\n\nimport pandas as pd\n\nfrom services.llm import request_to_chat_openai\n\n\ndef overview(config):\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/overview.txt\"\n\n    takeaways = pd.read_csv(f\"outputs/{dataset}/takeaways.csv\")\n    labels = pd.read_csv(f\"outputs/{dataset}/labels.csv\")\n\n    prompt = config[\"overview\"][\"prompt\"]\n    model = config[\"overview\"][\"model\"]\n\n    ids = labels[\"cluster-id\"].to_list()\n    takeaways.set_index(\"cluster-id\", inplace=True)\n    labels.set_index(\"cluster-id\", inplace=True)\n\n    input = \"\"\n    for i, id in enumerate(ids):\n        input += f\"# Cluster {i}/{len(ids)}: {labels.loc[id]['label']}\\n\\n\"\n        input += takeaways.loc[id][\"takeaways\"] + \"\\n\\n\"\n\n    messages = [{\"role\": \"user\", \"content\": prompt}, {\"role\": \"user\", \"content\": input}]\n    response = request_to_chat_openai(messages=messages, model=model)\n\n    with open(path, \"w\") as file:\n        file.write(response)\n",
        "prompt": "/system \n\n\u3042\u306a\u305f\u306f\u30b7\u30f3\u30af\u30bf\u30f3\u30af\u3067\u50cd\u304f\u5c02\u9580\u306e\u30ea\u30b5\u30fc\u30c1\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u3067\u3059\u3002\n\u30c1\u30fc\u30e0\u306f\u7279\u5b9a\u306e\u30c6\u30fc\u30de\u306b\u95a2\u3057\u3066\u30d1\u30d6\u30ea\u30c3\u30af\u30fb\u30b3\u30f3\u30b5\u30eb\u30c6\u30fc\u30b7\u30e7\u30f3\u3092\u5b9f\u65bd\u3057\u3001\u7570\u306a\u308b\u9078\u629e\u80a2\u306e\u30af\u30e9\u30b9\u30bf\u30fc\u3092\u5206\u6790\u3057\u59cb\u3081\u3066\u3044\u307e\u3059\u3002\n\u3053\u308c\u304b\u3089\u30af\u30e9\u30b9\u30bf\u30fc\u306e\u30ea\u30b9\u30c8\u3068\u305d\u306e\u7c21\u5358\u306a\u5206\u6790\u304c\u63d0\u4f9b\u3055\u308c\u307e\u3059\u3002\n\u3042\u306a\u305f\u306e\u4ed5\u4e8b\u306f\u3001\u8abf\u67fb\u7d50\u679c\u306e\u7c21\u6f54\u306a\u8981\u7d04\u3092\u8fd4\u3059\u3053\u3068\u3067\u3059\u3002\u8981\u7d04\u306f\u975e\u5e38\u306b\u7c21\u6f54\u306b\uff08\u6700\u5927\u30671\u6bb5\u843d\u3001\u6700\u59274\u6587\uff09\u307e\u3068\u3081\u3001\u7121\u610f\u5473\u306a\u8a00\u8449\u3092\u907f\u3051\u3066\u304f\u3060\u3055\u3044\u3002\n\u51fa\u529b\u306f\u65e5\u672c\u8a9e\u3067\u884c\u3063\u3066\u304f\u3060\u3055\u3044\u3002\n",
        "model": "gpt-3.5-turbo"
      }
    },
    {
      "step": "translation",
      "completed": "2024-11-20T11:33:21.674950",
      "duration": 0.001838,
      "params": {
        "languages": [],
        "flags": [],
        "source_code": "import json\n\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom tqdm import tqdm\n\nfrom utils import messages\n\nJAPANESE_UI_MAP = {\n    \"Argument\": \"\u8b70\u8ad6\",\n    # \"Original comment\": \"\u5143\u306e\u30b3\u30e1\u30f3\u30c8\",\n    \"Representative arguments\": \"\u4ee3\u8868\u7684\u306a\u8b70\u8ad6\",\n    \"Open full-screen map\": \"\u5168\u753b\u9762\u5730\u56f3\u3092\u958b\u304f\",\n    \"Back to report\": \"\u30ec\u30dd\u30fc\u30c8\u306b\u623b\u308b\",\n    \"Hide labels\": \"\u30e9\u30d9\u30eb\u3092\u975e\u8868\u793a\u306b\u3059\u308b\",\n    \"Show labels\": \"\u30e9\u30d9\u30eb\u3092\u8868\u793a\",\n    \"Show filters\": \"\u30d5\u30a3\u30eb\u30bf\u30fc\u3092\u8868\u793a\",\n    \"Hide filters\": \"\u30d5\u30a3\u30eb\u30bf\u30fc\u3092\u975e\u8868\u793a\",\n    \"Min. votes\": \"\u6700\u5c0f\u6295\u7968\u6570\",\n    \"Consensus\": \"\u30b3\u30f3\u30bb\u30f3\u30b5\u30b9\",\n    \"Showing\": \"\u8868\u793a\u4e2d\",\n    \"arguments\": \"\u8b70\u8ad6\",\n    \"Reset zoom\": \"\u30ba\u30fc\u30e0\u3092\u30ea\u30bb\u30c3\u30c8\",\n    \"Click anywhere on the map to close this\": \"\u3053\u306e\u30e1\u30c3\u30bb\u30fc\u30b8\u3092\u9589\u3058\u308b\u306b\u306f\u5730\u56f3\u306e\u3069\u3053\u304b\u3092\u30af\u30ea\u30c3\u30af\u3057\u3066\u304f\u3060\u3055\u3044\",\n    \"Click on the dot for details\": \"\u8a73\u7d30\u3092\u898b\u308b\u306b\u306f\u70b9\u3092\u30af\u30ea\u30c3\u30af\u3057\u3066\u304f\u3060\u3055\u3044\",\n    \"agree\": \"\u540c\u610f\u3059\u308b\",\n    \"disagree\": \"\u540c\u610f\u3057\u306a\u3044\",\n    \"Language\": \"\u8a00\u8a9e\",\n    \"English\": \"\u82f1\u8a9e\",\n    \"of total\": \"\u5408\u8a08\",\n    \"Overview\": \"\u5206\u6790\u7d50\u679c\u306e\u6982\u8981\",\n    \"Cluster analysis\": \"\u30af\u30e9\u30b9\u30bf\u30fc\u5206\u6790\",\n    \"Representative comments\": \"\u30b3\u30e1\u30f3\u30c8\u4f8b\",\n    \"Introduction\": \"\u5c0e\u5165\",\n    \"Clusters\": \"\u30af\u30e9\u30b9\u30bf\u30fc\",\n    \"Appendix\": \"\u4ed8\u9332\",\n    \"This report was generated using an AI pipeline that consists of the following steps\": \"\u3053\u306e\u30ec\u30dd\u30fc\u30c8\u306f\u3001\u4ee5\u4e0b\u306e\u30b9\u30c6\u30c3\u30d7\u3067\u69cb\u6210\u3055\u308c\u308bAI\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u3092\u4f7f\u7528\u3057\u3066\u751f\u6210\u3055\u308c\u307e\u3057\u305f\",\n    \"Step\": \"\u30b9\u30c6\u30c3\u30d7\",\n    \"extraction\": \"\u62bd\u51fa\",\n    \"show code\": \"\u30b3\u30fc\u30c9\u3092\u8868\u793a\",\n    \"hide code\": \"\u30b3\u30fc\u30c9\u3092\u975e\u8868\u793a\",\n    \"show prompt\": \"\u30d7\u30ed\u30f3\u30d7\u30c8\u3092\u8868\u793a\",\n    \"hide prompt\": \"\u30d7\u30ed\u30f3\u30d7\u30c8\u3092\u975e\u8868\u793a\",\n    \"embedding\": \"\u57cb\u3081\u8fbc\u307f\",\n    \"clustering\": \"\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\",\n    \"labelling\": \"\u30e9\u30d9\u30ea\u30f3\u30b0\",\n    \"takeaways\": \"\u307e\u3068\u3081\",\n    \"overview\": \"\u6982\u8981\",\n}\n\n\ndef translation(config):\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/translations.json\"\n    results = {}\n\n    languages = list(config.get(\"translation\", {}).get(\"languages\", []))\n    if len(languages) == 0:\n        print(\"No languages specified. Skipping translation step.\")\n        # creating an empty file any, to reduce special casing later\n        with open(path, \"w\") as file:\n            json.dump(results, file, indent=2)\n        return\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    labels = pd.read_csv(f\"outputs/{dataset}/labels.csv\")\n    takeaways = pd.read_csv(f\"outputs/{dataset}/takeaways.csv\")\n    with open(f\"outputs/{dataset}/overview.txt\") as f:\n        overview = f.read()\n\n    UI_copy = [\n        \"Argument\",\n        \"Original comment\",\n        \"Representative arguments\",\n        \"Open full-screen map\",\n        \"Back to report\",\n        \"Hide labels\",\n        \"Show labels\",\n        \"Show filters\",\n        \"Hide filters\",\n        \"Min. votes\",\n        \"Consensus\",\n        \"Showing\",\n        \"arguments\",\n        \"Reset zoom\",\n        \"Click anywhere on the map to close this\",\n        \"Click on the dot for details\",\n        \"agree\",\n        \"disagree\",\n        \"Language\",\n        \"English\",\n        \"arguments\",\n        \"of total\",\n        \"Overview\",\n        \"Cluster analysis\",\n        \"Representative comments\",\n        \"Introduction\",\n        \"Clusters\",\n        \"Appendix\",\n        \"This report was generated using an AI pipeline that consists of the following steps\",\n        \"Step\",\n        \"extraction\",\n        \"show code\",\n        \"hide code\",\n        \"show prompt\",\n        \"hide prompt\",\n        \"embedding\",\n        \"clustering\",\n        \"labelling\",\n        \"takeaways\",\n        \"overview\",\n    ]\n\n    japanese_ui = [JAPANESE_UI_MAP[key] for key in UI_copy]\n    arg_list = (\n        arguments[\"argument\"].to_list()\n        + labels[\"label\"].to_list()\n        + japanese_ui\n        + languages\n    )\n\n    if \"name\" in config:\n        arg_list.append(config[\"name\"])\n    if \"question\" in config:\n        arg_list.append(config[\"question\"])\n\n    prompt_file = config.get(\"translation_prompt\", \"default\")\n    with open(f\"prompts/translation/{prompt_file}.txt\") as f:\n        prompt = f.read()\n    model = config[\"model\"]\n\n    config[\"translation_prompt\"] = prompt\n\n    translations = [\n        translate_lang(arg_list, 10, prompt, lang, model) for lang in languages\n    ]\n\n    # handling long takeaways differently, WITHOUT batching too much\n    long_arg_list = takeaways[\"takeaways\"].to_list()\n    long_arg_list.append(overview)\n    if \"intro\" in config:\n        long_arg_list.append(config[\"intro\"])\n\n    long_translations = [\n        translate_lang(long_arg_list, 1, prompt, lang, model) for lang in languages\n    ]\n\n    for i, id in enumerate(arg_list):\n        print(\"i, id\", i, id)\n        results[str(id)] = list([t[i] for t in translations])\n    for i, id in enumerate(long_arg_list):\n        results[str(id)] = list([t[i] for t in long_translations])\n\n    with open(path, \"w\") as file:\n        json.dump(results, file, indent=2)\n\n\ndef translate_lang(arg_list, batch_size, prompt, lang, model):\n    translations = []\n    lang_prompt = prompt.replace(\"{language}\", lang)\n    print(f\"Translating to {lang}...\")\n    for i in tqdm(range(0, len(arg_list), batch_size)):\n        batch = arg_list[i : i + batch_size]\n        translations.extend(translate_batch(batch, lang_prompt, model))\n    return translations\n\n\ndef translate_batch(batch, lang_prompt, model, retries=3):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    input = json.dumps(list(batch))\n    response = llm(messages=messages(lang_prompt, input)).content.strip()\n    if \"```\" in response:\n        response = response.split(\"```\")[1]\n    if response.startswith(\"json\"):\n        response = response[4:]\n    try:\n        parsed = [a.strip() for a in json.loads(response)]\n        if len(parsed) != len(batch):\n            print(\"Warning: batch size mismatch!\")\n            print(\"Batch len:\", len(batch))\n            print(\"Response len:\", len(parsed))\n            for i, item in enumerate(batch):\n                print(f\"Batch item {i}:\", item)\n                if i < len(parsed):\n                    print(\"Response:\", parsed[i])\n            if len(batch) > 1:\n                print(\"Retrying with smaller batches...\")\n                mid = len(batch) // 2\n                return translate_batch(\n                    batch[:mid], lang_prompt, model, retries - 1\n                ) + translate_batch(batch[mid:], lang_prompt, model, retries - 1)\n            else:\n                print(\"Retrying batch...\")\n                return translate_batch(batch, lang_prompt, model, retries - 1)\n        else:\n            return parsed\n    except json.decoder.JSONDecodeError as e:\n        print(\"JSON error:\", e)\n        print(\"Response was:\", response)\n        if retries > 0:\n            print(\"Retrying batch...\")\n            return translate_batch(batch, lang_prompt, model, retries - 1)\n        else:\n            raise e\n",
        "prompt": "/system \n\n\u3042\u306a\u305f\u306f\u30d7\u30ed\u306e\u7ffb\u8a33\u8005\u3067\u3059\u3002\n\u82f1\u8a9e\u3067\u66f8\u304b\u308c\u305f\u5358\u8a9e\u3068\u6587\u7ae0\u306e\u30ea\u30b9\u30c8\u3092\u53d7\u3051\u53d6\u308a\u307e\u3059\u3002\n\u540c\u3058\u30ea\u30b9\u30c8\u3092\u540c\u3058\u9806\u756a\u3067\u3001\u65e5\u672c\u8a9e\u306b\u7ffb\u8a33\u3057\u3066\u8fd4\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u305f\u3060\u3057\u3001\u3082\u3057\u6587\u7ae0\u304c\u65e5\u672c\u8a9e\u3067\u66f8\u304b\u308c\u3066\u3044\u308b\u5834\u5408\u306f\u5143\u306e\u6587\u3092\u305d\u306e\u307e\u307e\u8fd4\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\u5143\u306e\u30ea\u30b9\u30c8\u3068\u540c\u3058\u9577\u3055\u306e\u6587\u5b57\u5217\u306e\u6709\u52b9\u306aJSON\u30ea\u30b9\u30c8\u3092\u8fd4\u3059\u3088\u3046\u306b\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\n",
        "model": "gpt-3.5-turbo"
      }
    },
    {
      "step": "aggregation",
      "completed": "2024-11-20T11:33:21.689680",
      "duration": 0.013079,
      "params": {
        "include_minor": true,
        "sampling_num": 5000,
        "title_in_map": null,
        "source_code": "\"\"\"Generate a convenient JSON output file.\"\"\"\nimport json\nfrom pathlib import Path\n\nimport pandas as pd\n\nROOT_DIR = Path(__file__).parent.parent.parent.parent\nCONFIG_DIR = ROOT_DIR / \"scatter\" / \"pipeline\" / \"configs\"\n\n\ndef create_custom_intro(config, total_sampled_num: int):\n    dataset = config[\"output_dir\"]\n    args_path = f\"outputs/{dataset}/args.csv\"\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n    result_path = f\"outputs/{dataset}/result.json\"\n\n    input_count = len(comments)\n    args_count = len(pd.read_csv(args_path))\n\n    print(f\"Input count: {input_count}\")\n    print(f\"Args count: {args_count}\")\n\n    base_custom_intro = \"\"\"{intro}\n\u5206\u6790\u5bfe\u8c61\u3068\u306a\u3063\u305f\u30c7\u30fc\u30bf\u306e\u4ef6\u6570\u306f{input_count}\u4ef6\u3067\u3001\u3053\u308c\u3089\u306e\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066OpenAI API\u3092\u7528\u3044\u3066{args_count}\u4ef6\u306e\u610f\u898b\uff08\u8b70\u8ad6\uff09\u3092\u62bd\u51fa\u3057\u3001\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u3092\u884c\u3063\u305f\u3002\n\"\"\"\n\n    intro = config[\"intro\"]\n    custom_intro = base_custom_intro.format(\n        intro=intro, input_count=input_count, args_count=args_count\n    )\n\n    if total_sampled_num < args_count:\n        extra_intro = \"\u306a\u304a\u3001\u30af\u30e9\u30b9\u30bf\u5206\u6790\u306b\u306f\u524d\u8ff0\u306e{args_count}\u4ef6\u306e\u30c7\u30fc\u30bf\u3092\u7528\u3044\u3066\u3044\u308b\u304c\u3001\u672c\u30da\u30fc\u30b8\u3067\u306f\u305d\u306e\u3046\u3061{total_sampled_num}\u4ef6\u306e\u30c7\u30fc\u30bf\u3092\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3057\u3066\u53ef\u8996\u5316\u3057\u3066\u3044\u308b\u3002\".format(\n            args_count=args_count, total_sampled_num=total_sampled_num\n        )\n        custom_intro += extra_intro\n    custom_intro += \"\u4e00\u90e8\u3001AI\u306b\u3088\u308b\u5206\u6790\u7d50\u679c\u306e\u4e2d\u3067\u3001\u4e8b\u5b9f\u3068\u7570\u306a\u308b\u5185\u5bb9\u306b\u3064\u3044\u3066\u306f\u524a\u9664\u3092\u884c\u3063\u305f\u3002\"\n    with open(result_path, \"r\") as f:\n        result = json.load(f)\n    result[\"config\"][\"intro\"] = custom_intro\n    with open(result_path, \"w\") as f:\n        json.dump(result, f, indent=2)\n\n\ndef aggregation(config):\n    path = f\"outputs/{config['output_dir']}/result.json\"\n    total_sampling_num = config[\"aggregation\"][\"sampling_num\"]\n    print(\"total sampling num:\", total_sampling_num)\n\n    results = {\n        \"clusters\": [],\n        \"comments\": {\"\": {}},\n        \"translations\": {},\n        \"overview\": \"\",\n        \"config\": config,\n    }\n\n    arguments = pd.read_csv(f\"outputs/{config['output_dir']}/args.csv\")\n    arguments.set_index(\"arg-id\", inplace=True)\n\n    languages = list(config.get(\"translation\", {}).get(\"languages\", []))\n    if len(languages) > 0:\n        with open(f\"outputs/{config['output_dir']}/translations.json\") as f:\n            translations = f.read()\n        results[\"translations\"] = json.loads(translations)\n\n    clusters = pd.read_csv(f\"outputs/{config['output_dir']}/clusters.csv\")\n    labels = pd.read_csv(f\"outputs/{config['output_dir']}/labels.csv\")\n    takeaways = pd.read_csv(f\"outputs/{config['output_dir']}/takeaways.csv\")\n    takeaways.set_index(\"cluster-id\", inplace=True)\n\n    print(\"relevant clusters score\")\n    print(clusters.sort_values(by=\"probability\", ascending=False).head(10))\n\n    clusters[\"x\"] = clusters[\"x\"].astype(float).round(4)\n    clusters[\"y\"] = clusters[\"y\"].astype(float).round(4)\n    clusters[\"probability\"] = clusters[\"probability\"].astype(float).round(1)\n\n    with open(f\"outputs/{config['output_dir']}/overview.txt\") as f:\n        overview = f.read()\n    results[\"overview\"] = overview\n\n    # \u30af\u30e9\u30b9\u30bf\u4e8b\u306b\u53ef\u8996\u5316\u3059\u308b\u30c7\u30fc\u30bf\u3092\u30b5\u30f3\u30d7\u30eb\u3059\u308b\n    # \u5404\u30af\u30e9\u30b9\u30bf\u306e\u4ef6\u6570\u306e\u5168\u4f53\u3067\u306e\u6bd4\u7387\u3092\u3082\u3068\u306b\u30b5\u30f3\u30d7\u30eb\u3059\u308b\n    arguments_num = len(arguments)\n    sample_rate = min(total_sampling_num / arguments_num, 1)\n    total_sampled_num = 0\n\n    sampled_comment_ids = []\n    for _, row in labels.iterrows():\n        cid = row[\"cluster-id\"]\n        label = row[\"label\"]\n        arg_rows = clusters[clusters[\"cluster-id\"] == cid]\n        c_arg_num = len(arg_rows)\n        sampling_num = int(c_arg_num * sample_rate)\n\n        if (\n            not config[\"aggregation\"][\"include_minor\"]\n            and sampling_num / total_sampling_num < 0.005\n        ):\n            continue\n        print(f\"sampling num: {sampling_num}\", c_arg_num, sample_rate)\n        total_sampled_num += sampling_num\n        arguments_in_cluster = []\n\n        # pickup top 5 for representative comments\n        sorted_rows = arg_rows.sort_values(by=\"probability\", ascending=False)\n        top_5 = sorted_rows.head(5)\n        for _, arg_row in top_5.head(sampling_num).iterrows():\n            arg_id = arg_row[\"arg-id\"]\n            try:\n                argument = arguments.loc[arg_id][\"argument\"]\n                comment_id = arguments.loc[arg_id][\"comment-id\"]\n                x = float(arg_row[\"x\"])\n                y = float(arg_row[\"y\"])\n                p = float(arg_row[\"probability\"])\n                obj = {\n                    \"arg_id\": arg_id,\n                    \"argument\": argument,\n                    \"comment_id\": str(comment_id),\n                    \"x\": x,\n                    \"y\": y,\n                    \"p\": p,\n                }\n                sampled_comment_ids.append(comment_id)\n                arguments_in_cluster.append(obj)\n            except:\n                print(\"Error with arg_id:\", arg_id)\n\n        results[\"clusters\"].append(\n            {\n                \"cluster\": label,\n                \"cluster_id\": str(cid),\n                \"takeaways\": takeaways.loc[cid][\"takeaways\"],\n                \"arguments\": arguments_in_cluster,\n            }\n        )\n\n        # random sampling\n        remaining = sorted_rows.iloc[5:]\n        remaining_sample_size = max(0, sampling_num - 5)\n        random_sample = remaining.sample(\n            n=min(remaining_sample_size, len(remaining)), random_state=42\n        )\n\n        for _, arg_row in random_sample.iterrows():\n            arg_id = arg_row[\"arg-id\"]\n            try:\n                argument = arguments.loc[arg_id][\"argument\"]\n                comment_id = arguments.loc[arg_id][\"comment-id\"]\n                x = float(arg_row[\"x\"])\n                y = float(arg_row[\"y\"])\n                p = float(arg_row[\"probability\"])\n                obj = {\n                    \"arg_id\": arg_id,\n                    \"argument\": argument,\n                    \"comment_id\": str(comment_id),\n                    \"x\": x,\n                    \"y\": y,\n                    \"p\": p,\n                }\n                sampled_comment_ids.append(comment_id)\n                arguments_in_cluster.append(obj)\n            except:\n                print(\"Error with arg_id:\", arg_id)\n\n    with open(path, \"w\") as file:\n        json.dump(results, file, indent=2)\n\n    create_custom_intro(config, total_sampled_num)\n"
      }
    },
    {
      "step": "visualization",
      "completed": "2024-11-20T11:33:28.516200",
      "duration": 6.825888,
      "params": {
        "replacements": [],
        "source_code": "import subprocess\n\n\ndef visualization(config):\n    output_dir = config[\"output_dir\"]\n    cwd = \"../next-app\"\n    command = f\"REPORT={output_dir} npm run build\"\n\n    try:\n        process = subprocess.Popen(\n            command,\n            shell=True,\n            cwd=cwd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            universal_newlines=True,\n        )\n        while True:\n            output_line = process.stdout.readline()\n            if output_line == \"\" and process.poll() is not None:\n                break\n            if output_line:\n                print(output_line.strip())\n        process.wait()\n        errors = process.stderr.read()\n        if errors:\n            print(\"Errors:\")\n            print(errors)\n    except subprocess.CalledProcessError as e:\n        print(\"Error: \", e)\n"
      }
    },
    {
      "step": "extraction",
      "completed": "2024-11-20T11:25:04.089529",
      "duration": 5.229545,
      "params": {
        "workers": 3,
        "limit": 12,
        "source_code": "import concurrent.futures\nimport json\nimport logging\nimport re\n\nimport pandas as pd\nfrom tqdm import tqdm\n\nfrom services.llm import request_to_chat_openai\nfrom utils import update_progress\n\nCOMMA_AND_SPACE_AND_RIGHT_BRACKET = re.compile(r\",\\s*(\\])\")\n\n\ndef extraction(config):\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/args.csv\"\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n\n    model = config[\"extraction\"][\"model\"]\n    prompt = config[\"extraction\"][\"prompt\"]\n    workers = config[\"extraction\"][\"workers\"]\n    limit = config[\"extraction\"][\"limit\"]\n\n    comment_ids = (comments[\"comment-id\"].values)[:limit]\n    comments.set_index(\"comment-id\", inplace=True)\n    results = pd.DataFrame()\n    update_progress(config, total=len(comment_ids))\n\n    existing_arguments = set()\n\n    for i in tqdm(range(0, len(comment_ids), workers)):\n        batch = comment_ids[i : i + workers]\n        batch_inputs = [comments.loc[id][\"comment-body\"] for id in batch]\n        batch_results = extract_batch(batch_inputs, prompt, model, workers)\n        for comment_id, extracted_args in zip(batch, batch_results):\n            for j, arg in enumerate(extracted_args):\n                if arg not in existing_arguments:\n                    new_row = {\n                        \"arg-id\": f\"A{comment_id}_{j}\",\n                        \"comment-id\": int(comment_id),\n                        \"argument\": arg,\n                    }\n                    results = pd.concat(\n                        [results, pd.DataFrame([new_row])], ignore_index=True\n                    )\n                    existing_arguments.add(arg)\n        update_progress(config, incr=len(batch))\n    results.to_csv(path, index=False)\n\n\nlogging.basicConfig(level=logging.ERROR)\n\n\ndef extract_batch(batch, prompt, model, workers):\n    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:\n        futures = [\n            executor.submit(extract_arguments, input, prompt, model)\n            for input in list(batch)\n        ]\n\n        done, not_done = concurrent.futures.wait(futures, timeout=30)\n\n        results = []\n\n        for future in not_done:\n            if not future.cancelled():\n                future.cancel()\n            results.append([])\n\n        for future in done:\n            try:\n                result = future.result()\n                results.append(result)\n            except Exception as e:\n                logging.error(f\"Task {future} failed with error: {e}\")\n                results.append([])\n\n        return results\n\n\ndef extract_by_llm(input, prompt, model):\n    messages = [\n        {\"role\": \"system\", \"content\": prompt},\n        {\"role\": \"user\", \"content\": input},\n    ]\n    response = request_to_chat_openai(messages=messages, model=model)\n    return response\n\n\ndef extract_arguments(input, prompt, model, retries=1):\n    messages = [\n        {\"role\": \"system\", \"content\": prompt},\n        {\"role\": \"user\", \"content\": input},\n    ]\n    try:\n        response = request_to_chat_openai(messages=messages, model=model, is_json=False)\n        response = (\n            COMMA_AND_SPACE_AND_RIGHT_BRACKET.sub(r\"\\1\", response)\n            .replace(\"```json\", \"\")\n            .replace(\"```\", \"\")\n        )\n        obj = json.loads(response)\n        # LLM sometimes returns valid JSON string\n        if isinstance(obj, str):\n            obj = [obj]\n        try:\n            items = [a.strip() for a in obj]\n        except Exception as e:\n            print(\"Error:\", e)\n            print(\"Input was:\", input)\n            print(\"Response was:\", response)\n            print(\"JSON was:\", obj)\n            print(\"skip\")\n            items = []\n        items = filter(None, items)  # omit empty strings\n        return items\n    except json.decoder.JSONDecodeError as e:\n        print(\"JSON error:\", e)\n        print(\"Input was:\", input)\n        print(\"Response was:\", response)\n        print(\"Silently giving up on trying to generate valid list.\")\n        return []\n",
        "prompt": "/system\n\u3042\u306a\u305f\u306f\u5c02\u9580\u7684\u306a\u30ea\u30b5\u30fc\u30c1\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u3067\u3001\u6574\u7406\u3055\u308c\u305f\u8b70\u8ad6\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u4f5c\u6210\u3059\u308b\u304a\u624b\u4f1d\u3044\u3092\u3059\u308b\u5f79\u5272\u3067\u3059\u3002\n\u4eba\u5de5\u77e5\u80fd\u306b\u95a2\u3059\u308b\u516c\u958b\u5354\u8b70\u3092\u5b9f\u65bd\u3057\u305f\u72b6\u6cc1\u3092\u60f3\u5b9a\u3057\u3066\u3044\u307e\u3059\u3002\u4e00\u822c\u5e02\u6c11\u304b\u3089\u5bc4\u305b\u3089\u308c\u305f\u8b70\u8ad6\u306e\u4f8b\u3092\u63d0\u793a\u3057\u307e\u3059\u306e\u3067\u3001\u305d\u308c\u3089\u3092\u3088\u308a\u7c21\u6f54\u3067\u8aad\u307f\u3084\u3059\u3044\u5f62\u306b\u6574\u7406\u3059\u308b\u304a\u624b\u4f1d\u3044\u3092\u304a\u9858\u3044\u3057\u307e\u3059\u3002\u5fc5\u8981\u306a\u5834\u5408\u306f2\u3064\u306e\u5225\u500b\u306e\u8b70\u8ad6\u306b\u5206\u5272\u3059\u308b\u3053\u3068\u3082\u3067\u304d\u307e\u3059\u304c\u3001\u591a\u304f\u306e\u5834\u5408\u306f1\u3064\u306e\u8b70\u8ad6\u306b\u307e\u3068\u3081\u308b\u65b9\u304c\u671b\u307e\u3057\u3044\u3067\u3057\u3087\u3046\u3002\n\u7d50\u679c\u306f\u6574\u5f62\u3055\u308c\u305fJSON\u5f62\u5f0f\u306e\u6587\u5b57\u5217\u30ea\u30b9\u30c8\u3068\u3057\u3066\u8fd4\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\u8981\u7d04\u306f\u5fc5\u305a\u65e5\u672c\u8a9e\u3067\u4f5c\u6210\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\n/human\n\nAI\u30c6\u30af\u30ce\u30ed\u30b8\u30fc\u306f\u3001\u305d\u306e\u30e9\u30a4\u30d5\u30b5\u30a4\u30af\u30eb\u5168\u4f53\u306b\u304a\u3051\u308b\u74b0\u5883\u8ca0\u8377\u3092\u524a\u6e1b\u3059\u308b\u3053\u3068\u306b\u7126\u70b9\u3092\u5f53\u3066\u3066\u958b\u767a\u3055\u308c\u308b\u3079\u304d\u3067\u3059\u3002\n\n/ai\n\n[\n\"AI\u30c6\u30af\u30ce\u30ed\u30b8\u30fc\u306e\u74b0\u5883\u8ca0\u8377\u524a\u6e1b\u306b\u7126\u70b9\u3092\u5f53\u3066\u308b\u3079\u304d\"\n]\n\n/human\n\nAI\u306e\u80fd\u529b\u3001\u9650\u754c\u3001\u502b\u7406\u7684\u8003\u616e\u4e8b\u9805\u306b\u3064\u3044\u3066\u3001\u4e00\u822c\u5e02\u6c11\u3092\u6559\u80b2\u3059\u308b\u5354\u8abf\u7684\u306a\u53d6\u308a\u7d44\u307f\u304c\u5fc5\u8981\u3067\u3059\u3002\n\n/ai\n\n[\n\"AI\u306e\u80fd\u529b\u306b\u3064\u3044\u3066\u4e00\u822c\u5e02\u6c11\u3092\u6559\u80b2\u3059\u3079\u304d\",\n\"AI\u306e\u9650\u754c\u3068\u502b\u7406\u7684\u8003\u616e\u4e8b\u9805\u306b\u3064\u3044\u3066\u4e00\u822c\u5e02\u6c11\u3092\u6559\u80b2\u3059\u3079\u304d\"\n]\n\n/human\n\nAI\u306f\u30b9\u30de\u30fc\u30c8\u30db\u30fc\u30e0\u3084\u30d3\u30eb\u306e\u30a8\u30cd\u30eb\u30ae\u30fc\u52b9\u7387\u3068\u5c45\u4f4f\u8005\u306e\u5feb\u9069\u6027\u3092\u6700\u9069\u5316\u3067\u304d\u307e\u3059\u3002\n\n/ai\n\n[\n\"AI\u306f\u30b9\u30de\u30fc\u30c8\u30db\u30fc\u30e0\u3084\u30d3\u30eb\u306e\u30a8\u30cd\u30eb\u30ae\u30fc\u52b9\u7387\u3068\u5c45\u4f4f\u8005\u306e\u5feb\u9069\u6027\u3092\u6700\u9069\u5316\u3067\u304d\u308b\"\n]\n\n/human\n\nAI\u306f\u30a8\u30cd\u30eb\u30ae\u30fc\u30b0\u30ea\u30c3\u30c9\u3092\u6700\u9069\u5316\u3057\u3001\u7121\u99c4\u3084\u70ad\u7d20\u6392\u51fa\u3092\u524a\u6e1b\u3067\u304d\u307e\u3059\u3002\n\n/ai\n\n[\n\"AI\u306f\u30a8\u30cd\u30eb\u30ae\u30fc\u30b0\u30ea\u30c3\u30c9\u3092\u6700\u9069\u5316\u3057\u3066\u7121\u99c4\u3068\u70ad\u7d20\u6392\u51fa\u3092\u524a\u6e1b\u3067\u304d\u308b\"\n]",
        "model": "gpt-3.5-turbo"
      }
    },
    {
      "step": "embedding",
      "completed": "2024-11-20T11:25:05.463605",
      "duration": 1.371581,
      "params": {
        "source_code": "import os\n\nimport pandas as pd\nfrom dotenv import load_dotenv\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain_openai import AzureOpenAIEmbeddings\nfrom tqdm import tqdm\n\nload_dotenv(\"../../.env\")\n\nEMBEDDING_MODEL = \"text-embedding-3-large\"\n\n\ndef embed_by_openai(args):\n    if os.getenv(\"USE_AZURE\"):\n        embeds = AzureOpenAIEmbeddings(\n            model=EMBEDDING_MODEL,\n            azure_endpoint=os.getenv(\"AZURE_EMBEDDING_ENDPOINT\"),\n        ).embed_documents(args)\n    else:\n        embeds = OpenAIEmbeddings(model=EMBEDDING_MODEL).embed_documents(args)\n    return embeds\n\n\ndef embedding(config):\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/embeddings.pkl\"\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    embeddings = []\n    batch_size = 1000\n    for i in tqdm(range(0, len(arguments), batch_size)):\n        args = arguments[\"argument\"].tolist()[i : i + batch_size]\n        embeds = embed_by_openai(args)\n        embeddings.extend(embeds)\n    df = pd.DataFrame(\n        [\n            {\"arg-id\": arguments.iloc[i][\"arg-id\"], \"embedding\": e}\n            for i, e in enumerate(embeddings)\n        ]\n    )\n    df.to_pickle(path)\n"
      }
    },
    {
      "step": "clustering",
      "completed": "2024-11-20T11:25:11.575914",
      "duration": 6.110666,
      "params": {
        "clusters": 3,
        "source_code": "\"\"\"Cluster the arguments using UMAP + HDBSCAN and GPT-4.\"\"\"\n\nfrom importlib import import_module\n\nimport numpy as np\nimport pandas as pd\nfrom janome.tokenizer import Tokenizer\n\nSTOP_WORDS = [\n    \"\u306e\",\n    \"\u306b\",\n    \"\u306f\",\n    \"\u3092\",\n    \"\u305f\",\n    \"\u304c\",\n    \"\u3067\",\n    \"\u3066\",\n    \"\u3068\",\n    \"\u3057\",\n    \"\u308c\",\n    \"\u3055\",\n    \"\u3042\u308b\",\n    \"\u3044\u308b\",\n    \"\u3082\",\n    \"\u3059\u308b\",\n    \"\u304b\u3089\",\n    \"\u306a\",\n    \"\u3053\u3068\",\n    \"\u3068\u3057\u3066\",\n    \"\u3044\u304f\",\n    \"\u306a\u3044\",\n]\nTOKENIZER = Tokenizer()\n\n\ndef clustering(config):\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/clusters.csv\"\n    arguments_df = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    arguments_array = arguments_df[\"argument\"].values\n\n    embeddings_df = pd.read_pickle(f\"outputs/{dataset}/embeddings.pkl\")\n    embeddings_array = np.asarray(embeddings_df[\"embedding\"].values.tolist())\n    clusters = config[\"clustering\"][\"clusters\"]\n\n    result = cluster_embeddings(\n        docs=arguments_array,\n        embeddings=embeddings_array,\n        metadatas={\n            \"arg-id\": arguments_df[\"arg-id\"].values,\n            \"comment-id\": arguments_df[\"comment-id\"].values,\n        },\n        min_cluster_size=clusters,\n        n_topics=clusters,\n    )\n    result.to_csv(path, index=False)\n\n\ndef tokenize_japanese(text):\n    return [\n        token.surface\n        for token in TOKENIZER.tokenize(text)\n        if token.surface not in STOP_WORDS\n    ]\n\n\ndef cluster_embeddings(\n    docs,\n    embeddings,\n    metadatas,\n    min_cluster_size=2,\n    n_components=2,\n    n_topics=6,\n):\n    # (!) we import the following modules dynamically for a reason\n    # (they are slow to load and not required for all pipelines)\n    SpectralClustering = import_module(\"sklearn.cluster\").SpectralClustering\n    HDBSCAN = import_module(\"hdbscan\").HDBSCAN\n    UMAP = import_module(\"umap\").UMAP\n    CountVectorizer = import_module(\"sklearn.feature_extraction.text\").CountVectorizer\n    BERTopic = import_module(\"bertopic\").BERTopic\n\n    umap_model = UMAP(\n        random_state=42,\n        n_components=n_components,\n    )\n    hdbscan_model = HDBSCAN(min_cluster_size=min_cluster_size)\n\n    vectorizer_model = CountVectorizer(tokenizer=tokenize_japanese)\n    topic_model = BERTopic(\n        umap_model=umap_model,\n        hdbscan_model=hdbscan_model,\n        vectorizer_model=vectorizer_model,\n        verbose=True,\n    )\n\n    # Fit the topic model.\n    _, __ = topic_model.fit_transform(docs, embeddings=embeddings)\n\n    n_samples = len(embeddings)\n    n_neighbors = min(n_samples - 1, 10)\n    spectral_model = SpectralClustering(\n        n_clusters=n_topics,\n        affinity=\"nearest_neighbors\",\n        n_neighbors=n_neighbors,  # Use the modified n_neighbors\n        random_state=42,\n    )\n    umap_embeds = umap_model.fit_transform(embeddings)\n    cluster_labels = spectral_model.fit_predict(umap_embeds)\n\n    result = topic_model.get_document_info(\n        docs=docs,\n        metadata={\n            **metadatas,\n            \"x\": umap_embeds[:, 0],\n            \"y\": umap_embeds[:, 1],\n        },\n    )\n\n    result.columns = [c.lower() for c in result.columns]\n    result = result[[\"arg-id\", \"x\", \"y\", \"probability\"]]\n    result[\"cluster-id\"] = cluster_labels\n\n    return result\n"
      }
    },
    {
      "step": "takeaways",
      "completed": "2024-11-20T11:25:24.617977",
      "duration": 6.983679,
      "params": {
        "sample_size": 30,
        "source_code": "\"\"\"Create summaries for the clusters.\"\"\"\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nfrom services.llm import request_to_chat_openai\nfrom utils import update_progress\n\n\ndef takeaways(config):\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/takeaways.csv\"\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    clusters = pd.read_csv(f\"outputs/{dataset}/clusters.csv\")\n\n    results = pd.DataFrame()\n\n    sample_size = config[\"takeaways\"][\"sample_size\"]\n    prompt = config[\"takeaways\"][\"prompt\"]\n    model = config[\"takeaways\"][\"model\"]\n\n    model = config.get(\"model_takeaways\", config.get(\"model\", \"gpt3.5-turbo\"))\n    cluster_ids = clusters[\"cluster-id\"].unique()\n\n    update_progress(config, total=len(cluster_ids))\n\n    for _, cluster_id in tqdm(enumerate(cluster_ids), total=len(cluster_ids)):\n        args_ids = clusters[clusters[\"cluster-id\"] == cluster_id][\"arg-id\"].values\n        args_ids = np.random.choice(\n            args_ids, size=min(len(args_ids), sample_size), replace=False\n        )\n        args_sample = arguments[arguments[\"arg-id\"].isin(args_ids)][\"argument\"].values\n        label = generate_takeaways(args_sample, prompt, model)\n        results = pd.concat(\n            [results, pd.DataFrame([{\"cluster-id\": cluster_id, \"takeaways\": label}])],\n            ignore_index=True,\n        )\n        update_progress(config, incr=1)\n\n    results.to_csv(path, index=False)\n\n\ndef generate_takeaways(args_sample, prompt, model):\n    input = \"\\n\".join(args_sample)\n    messages = [{\"role\": \"user\", \"content\": prompt}, {\"role\": \"user\", \"content\": input}]\n    response = request_to_chat_openai(messages=messages, model=model)\n    return response\n",
        "prompt": "/system \n\n\u3042\u306a\u305f\u306f\u30d7\u30ed\u306e\u30ea\u30b5\u30fc\u30c1\u30fb\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u3067\u3001\u79c1\u306e\u4ed5\u4e8b\u3092\u624b\u4f1d\u3046\u3053\u3068\u304c\u3042\u306a\u305f\u306e\u4ed5\u4e8b\u3067\u3059\u3002\nX\u4e0a\u306e\u30dd\u30b9\u30c8\u306e\u30ea\u30b9\u30c8\u3092\u4e0e\u3048\u308b\u306e\u3067\u3001\u305d\u308c\u3089\u306e\u4e3b\u8981\u306a\u30dd\u30a4\u30f3\u30c8\u3092\u4e00\u3064\u304b\u4e8c\u3064\u306e\u6bb5\u843d\u3067\u307e\u3068\u3081\u3066\u304f\u3060\u3055\u3044\u3002\n\u5fdc\u7b54\u30c6\u30ad\u30b9\u30c8\u306f\u3001\u7c21\u6f54\u3067\u3001\u77ed\u304f\u3001\u8aad\u307f\u3084\u3059\u3044\u6587\u7ae0\u306b\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\u51fa\u529b\u306f\u5fc5\u305a\u65e5\u672c\u8a9e\u3067\u884c\u3063\u3066\u304f\u3060\u3055\u3044\u3002\n \n/human\n\n[\n  \"\u79c1\u306f\u9283\u66b4\u529b\u304c\u6211\u3005\u306e\u793e\u4f1a\u306b\u304a\u3051\u308b\u6df1\u523b\u306a\u516c\u8846\u885b\u751f\u306e\u5371\u6a5f\u3067\u3042\u308b\u3068\u5f37\u304f\u4fe1\u3058\u3066\u3044\u307e\u3059\u3002\",\n  \"\u3053\u306e\u554f\u984c\u3092\u5305\u62ec\u7684\u306a\u9283\u898f\u5236\u306b\u3088\u3063\u3066\u7dca\u6025\u306b\u5bfe\u51e6\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\",\n  \"\u79c1\u306f\u5168\u3066\u306e\u9283\u8cfc\u5165\u8005\u306b\u5bfe\u3059\u308b\u30e6\u30cb\u30d0\u30fc\u30b5\u30eb\u80cc\u666f\u8abf\u67fb\u306e\u5b9f\u65bd\u3092\u652f\u6301\u3057\u3066\u3044\u307e\u3059\u3002\",\n  \"\u79c1\u306f\u30a2\u30b5\u30eb\u30c8\u6b66\u5668\u3068\u5927\u5bb9\u91cf\u30de\u30ac\u30b8\u30f3\u306e\u7981\u6b62\u306b\u8cdb\u6210\u3067\u3059\u3002\",\n  \"\u9055\u6cd5\u306a\u9283\u306e\u5bc6\u58f2\u3092\u9632\u3050\u305f\u3081\u306b\u3001\u3088\u308a\u53b3\u3057\u3044\u898f\u5236\u3092\u6c42\u3081\u3066\u3044\u307e\u3059\u3002\",\n  \"\u9283\u8cfc\u5165\u306e\u904e\u7a0b\u3067\u7cbe\u795e\u7684\u5065\u5eb7\u8a55\u4fa1\u3092\u5fc5\u9808\u306b\u3059\u3079\u304d\u3060\u3068\u4e3b\u5f35\u3057\u3066\u3044\u307e\u3059\u3002\"\n]\n/ai \n\n\u53c2\u52a0\u8005\u305f\u3061\u306f\u3001\u9283\u66b4\u529b\u304c\u6df1\u523b\u306a\u793e\u4f1a\u554f\u984c\u3067\u3042\u308b\u3068\u8a8d\u8b58\u3057\u3001\u3053\u308c\u306b\u5bfe\u51e6\u3059\u308b\u305f\u3081\u306b\u5305\u62ec\u7684\u306a\u9283\u898f\u5236\u306e\u5c0e\u5165\u3092\u5f37\u304f\u6c42\u3081\u307e\u3057\u305f\u3002\u5177\u4f53\u7684\u306b\u306f\u3001\u5168\u3066\u306e\u9283\u8cfc\u5165\u8005\u306b\u5bfe\u3059\u308b\u80cc\u666f\u8abf\u67fb\u306e\u7fa9\u52d9\u5316\u3084\u3001\u30a2\u30b5\u30eb\u30c8\u6b66\u5668\u3068\u5927\u5bb9\u91cf\u30de\u30ac\u30b8\u30f3\u306e\u7981\u6b62\u3001\u9055\u6cd5\u306a\u9283\u53d6\u5f15\u306e\u53d6\u308a\u7de0\u307e\u308a\u5f37\u5316\u3001\u305d\u3057\u3066\u9283\u8cfc\u5165\u6642\u306e\u7cbe\u795e\u5065\u5eb7\u8a55\u4fa1\u3092\u512a\u5148\u7684\u306b\u5c0e\u5165\u3059\u3079\u304d\u3060\u3068\u3057\u3066\u3044\u307e\u3059\u3002\n",
        "model": "gpt-3.5-turbo"
      }
    }
  ],
  "end_time": "2024-12-30T13:16:41.766351",
  "error": "TypeError: Client.__init__() got an unexpected keyword argument 'proxies'",
  "error_stack_trace": "Traceback (most recent call last):\n  File \"/home/ubuntu/repos/anno-broadlistening/scatter/pipeline/main.py\", line 23, in main\n    run_step(\"embedding\", embedding, config)\n  File \"/home/ubuntu/repos/anno-broadlistening/scatter/pipeline/utils.py\", line 259, in run_step\n    func(config)\n  File \"/home/ubuntu/repos/anno-broadlistening/scatter/pipeline/steps/embedding.py\", line 46, in embedding\n    embeds = embed_by_openai(args, model)\n  File \"/home/ubuntu/repos/anno-broadlistening/scatter/pipeline/steps/embedding.py\", line 32, in embed_by_openai\n    embeds = OpenAIEmbeddings(model=model).embed_documents(args)\n  File \"/home/ubuntu/.pyenv/versions/3.10.15/lib/python3.10/site-packages/langchain_core/_api/deprecation.py\", line 216, in warn_if_direct_instance\n    return wrapped(self, *args, **kwargs)\n  File \"/home/ubuntu/.pyenv/versions/3.10.15/lib/python3.10/site-packages/pydantic/main.py\", line 214, in __init__\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n  File \"/home/ubuntu/.pyenv/versions/3.10.15/lib/python3.10/site-packages/pydantic/_internal/_decorators_v1.py\", line 148, in _wrapper1\n    return validator(values)\n  File \"/home/ubuntu/.pyenv/versions/3.10.15/lib/python3.10/site-packages/langchain_core/utils/pydantic.py\", line 219, in wrapper\n    return func(cls, values)\n  File \"/home/ubuntu/.pyenv/versions/3.10.15/lib/python3.10/site-packages/langchain_community/embeddings/openai.py\", line 354, in validate_environment\n    values[\"client\"] = openai.OpenAI(**client_params).embeddings\n  File \"/home/ubuntu/.pyenv/versions/3.10.15/lib/python3.10/site-packages/openai/_client.py\", line 123, in __init__\n    super().__init__(\n  File \"/home/ubuntu/.pyenv/versions/3.10.15/lib/python3.10/site-packages/openai/_base_client.py\", line 856, in __init__\n    self._client = http_client or SyncHttpxClientWrapper(\n  File \"/home/ubuntu/.pyenv/versions/3.10.15/lib/python3.10/site-packages/openai/_base_client.py\", line 754, in __init__\n    super().__init__(**kwargs)\nTypeError: Client.__init__() got an unexpected keyword argument 'proxies'\n"
}